<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Hongxu Xu</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PMJ5E1FYG6"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
        
          gtag('config', 'G-PMJ5E1FYG6');
        </script>
        <meta name="description" content="Hongxu Xu&#x27;s personal website">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">
        <meta property="og:title" content="print.md" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="style.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>‚Üê</kbd> or <kbd>‚Üí</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Hongxu Xu</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="hongxu-xu"><a class="header" href="#hongxu-xu">Hongxu Xu</a></h1>
<img class="right circle" src="https://avatars.githubusercontent.com/u/2201482?v=4" alt="Hongxu Xu" width="160px">
<p><strong>PhD Student (May 2025 ‚Äì Present)</strong><br />
Supervised by <a href="https://cs.uwaterloo.ca/~cnsun">Prof. Chengnian Sun</a><br />
<a href="https://cs.uwaterloo.ca/">Cheriton School of Computer Science</a><br />
<a href="https://uwaterloo.ca/">University of Waterloo</a>, Canada</p>
<p><strong>B.Sc. in Computer Science (Sep 2014 ‚Äì Jun 2018)</strong><br />
<a href="https://www.bnu.edu.cn/">Beijing Normal University</a>, China</p>
<h2 id="research-interests"><a class="header" href="#research-interests">Research Interests</a></h2>
<ul>
<li>Software Engineering</li>
<li>Programming Languages</li>
</ul>
<p>without a focus yet‚Ä¶</p>
<h2 id="socials"><a class="header" href="#socials">Socials</a></h2>
<ul>
<li>GitHub: <a href="https://github.com/xuhongxu96">https://github.com/xuhongxu96</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/xuhongxu/">https://www.linkedin.com/in/xuhongxu/</a></li>
<li>Email: h4Ô∏è‚É£4Ô∏è‚É£5Ô∏è‚É£xu üåÄ uwaterloo.ca</li>
</ul>
<p>If you‚Äôd like to hear more about my thoughts and life, consider following me on:</p>
<img class="right" src="wechat_mp.jpg" alt="WeChat MP" width="160px">
<ul>
<li>
<p>Substack: <a href="https://hongxuxu.substack.com/">https://hongxuxu.substack.com/</a> (<strong>English only</strong>)<br />
<em>Subscribe to my newsletter on Substack to get my latest updates.</em></p>
</li>
<li>
<p>ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑: xuhongxu_it (<strong>Chinese only</strong>).<br />
<em><strong>Scan the QR code on the right</strong> to follow my WeChat Official Account</em>.</p>
</li>
</ul>
<h2 id="misc"><a class="header" href="#misc">Misc.</a></h2>
<ul>
<li>Chinese Name: ËÆ∏ÂÆèÊó≠</li>
<li>Pronunciation: <code>Hong-shyoo</code> (IPA: <code>[x ä≈ã …ïy]</code>)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="publications"><a class="header" href="#publications">Publications</a></h1>
<h2 id="papers"><a class="header" href="#papers">Papers</a></h2>
<p>Nothing yet‚Ä¶</p>
<h2 id="books"><a class="header" href="#books">Books</a></h2>
<ul>
<li><code>Feb 2024</code> <a href="https://book.douban.com/subject/36787652/">CMakeÊûÑÂª∫ÂÆûÊàòÔºöÈ°πÁõÆÂºÄÂèëÂç∑ (CMake Build Practice: Project Development Volume)</a><br />
Published by <a href="https://www.ptpress.com.cn/">‰∫∫Ê∞ëÈÇÆÁîµÂá∫ÁâàÁ§æ (Posts &amp; Telecom Press, China)</a><br />
Produced by <a href="https://www.epubit.com/">ÂºÇÊ≠•Âõæ‰π¶ (epubit)</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="experience"><a class="header" href="#experience">Experience</a></h1>
<h2 id="phd-student-may-2025--present"><a class="header" href="#phd-student-may-2025--present">PhD Student (May 2025 ‚Äì Present)</a></h2>
<p>Supervised by <a href="https://cs.uwaterloo.ca/~cnsun">Prof. Chengnian Sun</a></p>
<p><a href="https://cs.uwaterloo.ca/">Cheriton School of Computer Science</a><br />
<a href="https://uwaterloo.ca/">University of Waterloo</a>, Canada</p>
<h2 id="senior-sde-sep-2022--mar-2025"><a class="header" href="#senior-sde-sep-2022--mar-2025">Senior SDE (Sep 2022 ‚Äì Mar 2025)</a></h2>
<p><a href="https://seed.bytedance.com/en/direction/speech">Seed/Data Speech Team</a><br />
<a href="https://www.bytedance.com/en/">ByteDance</a>, Shanghai, China</p>
<p><em>Led the development of the Text-to-Speech engine and contributed to the Doubao AI assistant application.</em></p>
<!-- - Led the architectural design and development of the Text-to-Speech
  front-end engine, overseeing its overall system integration.
- Developed a streaming Markdown parser, based on the cmark open-source
  library, capable of real-time, character-wise parsing of Markdown
  generated by large language models. Authored a patent for this work,
  which has been adopted in the Doubao/Cici AI assistant application.
- Designed and implemented a cross-lingual [SSML](https://www.w3.org/TR/speech-synthesis11/) framework that lowers
  SSML abstract syntax trees to an intermediate representation,
  facilitating flexible modification of synthesized speech across
  multiple languages.
- Enhanced CI/CD pipelines and testing infrastructure by integrating
  sanitizers and static analyzers into continuous integration workflows.
- Collaborated with research teams to engineer and deploy
  state-of-the-art speech AI models based on large language models. -->
<h2 id="sde-2-aug-2021--sep-2022"><a class="header" href="#sde-2-aug-2021--sep-2022">SDE-2 (Aug 2021 ‚Äì Sep 2022)</a></h2>
<p>MSAI Team<br />
<a href="https://www.microsoft.com/en-us/aprd/aboutus/team-stca">Microsoft STC-Asia</a>, Suzhou, China</p>
<p><em>Led the development of Microsoft WordBreaker and initiated a modern NLP toolkit for Office 365.</em></p>
<!-- - Led the development of Microsoft WordBreaker and initiated a 
  modern workflow-based NLP toolkit for Office 365.
- Enhanced Microsoft 365 Search (serving Outlook, Teams, SharePoint, etc.) 
  in global markets by systematically resolving internationalization issues. -->
<h2 id="sde-jul-2018--aug-2021"><a class="header" href="#sde-jul-2018--aug-2021">SDE (Jul 2018 ‚Äì Aug 2021)</a></h2>
<p>MSAI Team<br />
<a href="https://www.microsoft.com/en-us/aprd/aboutus/team-stca">Microsoft STC-Asia</a><br />
Beijing, China (Relocated to Suzhou, Jiangsu, China in May 2019)</p>
<p><em>Worked on 20-year-old Microsoft WordBreaker.</em></p>
<!-- - Maintained Microsoft WordBreaker, 
  multilingual text segmentation tool used in Bing and Office,
  which has 20 years of history, 
  and includes source code written in both C++98 and C++17.
- Improved Korean WordBreaker performance by >20% through
  stateless refactoring and replacing regex with an
  automaton-based rule engine. -->
<h3 id="short-term-contributor-nov-2020--jan-2021"><a class="header" href="#short-term-contributor-nov-2020--jan-2021">Short-Term Contributor (Nov 2020 ‚Äì Jan 2021)</a></h3>
<p>Windows APS Team (temporary assignment)<br />
<a href="https://www.microsoft.com/en-us/aprd/aboutus/team-stca">Microsoft STC-Asia</a>, Suzhou, China</p>
<p><em>Contributed to the formation of the new team, and Windows 11 application development.</em></p>
<!-- - Authored design document templates and established DevOps processes to 
  facilitate the formation of a new team for Windows 11 application 
  development. Led initial review meetings to ensure effective onboarding.
- Contributed to the Microsoft Calculator project by encapsulating the 
  native rational calculation library as a cross-platform C# package.
- Assisted in migrating components of the Calculator application from 
  C++/CX to C# using ANTLR 4, a parser generator. -->
<h2 id="bsc-in-computer-science-sep-2014--jun-2018"><a class="header" href="#bsc-in-computer-science-sep-2014--jun-2018">B.Sc. in Computer Science (Sep 2014 ‚Äì Jun 2018)</a></h2>
<p><a href="https://www.bnu.edu.cn/">Beijing Normal University</a>, China</p>
<h3 id="sde-intern-jul-2017--dec-2017"><a class="header" href="#sde-intern-jul-2017--dec-2017">SDE Intern (Jul 2017 ‚Äì Dec 2017)</a></h3>
<p>Bing Search Relevance Team<br />
<a href="https://www.microsoft.com/en-us/aprd/aboutus/team-stca">Microsoft STC-Asia</a>, Beijing, China</p>
<p><em>Answer triggering model development for Bing Search.</em></p>
<!-- - Created a CFG (Context-Free Grammar) parser 
  to reduce the complexity of the rules 
  for answer triggering models used in Bing by
  automaton minimization. 
  Reduced the latency by 20%. -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="awards"><a class="header" href="#awards">Awards</a></h1>
<ul>
<li>Outstanding Graduate, Beijing Normal University, 2018</li>
<li>Top Ten Volunteer, Beijing Normal University, 2015</li>
<li>First Prize, National Olympiad in Informatics in Provinces (NOIP), 2013</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="teaching"><a class="header" href="#teaching">Teaching</a></h1>
<ul>
<li>Instructional Apprentice, <a href="https://student.cs.uwaterloo.ca/~cs246/F25/index.shtml">CS 246 - Object-Oriented Software Development</a>, Fall 2025</li>
<li>Teaching Assistant, <a href="https://student.cs.uwaterloo.ca/~cs246/S25/index.shtml">CS 246 - Object-Oriented Software Development</a>, Spring 2025</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="my-thoughts-and-life"><a class="header" href="#my-thoughts-and-life">My Thoughts and Life</a></h1>
<p>If you‚Äôd like to hear more about my thoughts and life, consider following me on:</p>
<img class="right" src="wechat_mp.jpg" alt="WeChat MP" width="160px">
<ul>
<li>
<p>Substack: <a href="https://hongxuxu.substack.com/">https://hongxuxu.substack.com/</a> (<strong>English only</strong>)<br />
<em>Subscribe to my newsletter on Substack to get my latest updates.</em></p>
</li>
<li>
<p>ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑: xuhongxu_it (<strong>Chinese only</strong>).<br />
<em><strong>Scan the QR code on the right</strong> to follow my WeChat Official Account</em>.</p>
</li>
</ul>
<h2 id="selected-substack-posts"><a class="header" href="#selected-substack-posts">Selected Substack Posts</a></h2>
<div class="substack-post-embed"><p lang="en">My 2025 by Hongxu Xu</p><p></p><a data-post-link href="https://hongxuxu.substack.com/p/my-2025">Read on Substack</a></div><script async src="https://substack.com/embedjs/embed.js" charset="utf-8"></script>
<div class="substack-post-embed"><p lang="en">G1 Pain Points by Hongxu Xu</p><p>Ontario Drive Test</p><a data-post-link href="https://hongxuxu.substack.com/p/g1-pain-points">Read on Substack</a></div><script async src="https://substack.com/embedjs/embed.js" charset="utf-8"></script><div style="break-before: page; page-break-before: always;"></div><h1 id="cs-452652-real-time-programming-the-train-course"><a class="header" href="#cs-452652-real-time-programming-the-train-course">CS 452/652: Real-Time Programming (the Train Course)</a></h1>
<blockquote>
<p>Spring 25</p>
</blockquote>
<p>I <em>dropped</em> this course after I learned that it is really <em>time-consuming</em>.</p>
<p>However, before I dropped it, I did some research and found some useful resources,
which I summarized in <a href="https://git.uwaterloo.ca/h445xu/cs652">MarklinSim All-in-One</a> repo.</p>
<p>In short, follow the readme in the repo to set up:</p>
<ol>
<li>MarklinSim (an emulator for the train kit)</li>
<li>QEMU (an emulator for the Raspberry Pi 4B)</li>
<li>Example Image (your real-time system).</li>
</ol>
<p>I believe it will save you a lot of time. Good luck!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cs-480680-introduction-to-machine-learning"><a class="header" href="#cs-480680-introduction-to-machine-learning">CS 480/680: Introduction to Machine Learning</a></h1>
<blockquote>
<p>Spring 25</p>
</blockquote>
<p>This is not a hard course.</p>
<p><a href="notes/final-review.pdf">Download PDF</a></p>
<pre><code class="language-latex">\documentclass{article}
\usepackage{fullpage}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url, hyperref}
\usepackage{algorithm2e}
\usepackage[margin=0.25in]{geometry}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.18}

\usepgfplotslibrary{external}
\tikzexternalize

\lstset{basicstyle=\fontfamily{pcr}\footnotesize}

\graphicspath{./}

\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\iprod}[1]{\left\langle #1 \right\rangle}

\newcommand*\MY@rightharpoonupfill@{%
    \arrowfill@\relbar\relbar\rightharpoonup
}
\newcommand*\overrightharpoon{%
    \mathpalette{\overarrow@\MY@rightharpoonupfill@}%
}

\theoremstyle{definition}
\newtheorem{question}{Question}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Var}{Var}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\llnorm}[1]{\left\| #1 \right\|_2}
\newcommand{\mnorm}[1]{\left\| #1 \right\|_1}
\newcommand{\fnorm}[1]{\left\| #1 \right\|_F}

\title{\large CS480/680, Spring 2025\\\huge Review Notes}

\author{}
\date{\today}
\setlength\parindent{0pt}

%\input{preamble}

\begin{document}

\maketitle

\newpage

\part{For Mid-Term}

\section{Perceptron}

\begin{question}[Linear Function]
    \begin{equation}
        \begin{split}
            &amp; \forall \alpha \beta \in \mathbb{R}, \forall \boldsymbol{x}, \boldsymbol{z} \in \mathbb{R}^d, 
            f(\alpha \boldsymbol{x} + \beta \boldsymbol{z}) = \alpha \cdot f(\boldsymbol{X}) + \beta \cdot f(\boldsymbol{z}) \\
            \Longleftrightarrow
            &amp; \exists \boldsymbol{w} \in \mathbb{R}^d, f(\boldsymbol{x}) = \iprod{\boldsymbol{x}, \boldsymbol{w}}
        \end{split}
    \end{equation}

    \textbf{Proof $\Rightarrow$:}
    Let $\boldsymbol{w} \coloneq \left[ f(\boldsymbol{e_1}), \dots, f(\boldsymbol{e_d}) \right]^T$, where
    $\boldsymbol{e_i}$ is the $i$-th coordinate vector.
    \begin{equation}
        \begin{split}
            f(\boldsymbol{x}) &amp; = f( x_1 \boldsymbol{e_1} + \dots + x_d \boldsymbol{e_d} ) \\
            &amp; = x_1 f(\boldsymbol{e_1}) + \dots + x_d f(\boldsymbol{e_d}) \\
            &amp; = \iprod{\boldsymbol{x}, \boldsymbol{w}}
        \end{split}
    \end{equation}

    \textbf{Proof $\Leftarrow$:}
    \begin{equation}
        \begin{split}
            f(\alpha \boldsymbol{x} + \beta \boldsymbol{z}) &amp; = \iprod{\alpha \boldsymbol{x} + \beta \boldsymbol{z}, \boldsymbol{w}} \\
            &amp; = \iprod{\alpha \boldsymbol{x}, \boldsymbol{w}} + \iprod{\beta \boldsymbol{z}, \boldsymbol{w}} \\
            &amp; = \alpha \iprod{\boldsymbol{x}, \boldsymbol{w}} + \beta \iprod{\boldsymbol{z}, \boldsymbol{w}} \\
            &amp; = \alpha f(\boldsymbol{x}) + \beta f(\boldsymbol{z})
        \end{split}
    \end{equation}
\end{question}

\begin{question}[$\boldsymbol{w}$ is Orthogonal to Decision Boundary $H$]
    Any vector on $H$ can be written as $\overrightharpoon{\boldsymbol{x}\boldsymbol{x'}} = \boldsymbol{x'} - \boldsymbol{x}$,
    \begin{equation}
            \iprod{\boldsymbol{x'} - \boldsymbol{x}, \boldsymbol{w}} 
            = \iprod{\boldsymbol{x'}, \boldsymbol{w}} - \iprod{\boldsymbol{x}, \boldsymbol{w}}
            = -b - (-b) = 0
    \end{equation}
\end{question}

\begin{question}[Update Rule for Perceptron]
\begin{equation} \label{perceptron-update-rule}
    \begin{split}
        \boldsymbol{w} \leftarrow \boldsymbol{w} + y_i \boldsymbol{x_i} \\
        b \leftarrow = b + y_i
    \end{split}
\end{equation}
\end{question}

\begin{question}[Feasibility of Perceptron]
The goal is to find $\boldsymbol{w} \in \mathbb{R}^d, b \in \mathbb{R}$, 
such that $\forall i, y_i \paren{\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b} &gt; 0$.

According to the update rule \ref{perceptron-update-rule},
\begin{equation}
    \begin{split}
        y\left[ \iprod{\boldsymbol{x}, \boldsymbol{w_{new}}} + b_{new} \right]
        &amp;= y\left[ \iprod{\boldsymbol{x}, \boldsymbol{w_{old} + y \boldsymbol{x}}} + b_{old} + y \right] \\
        &amp;= y\left[ \iprod{\boldsymbol{x}, \boldsymbol{w_{old}}} + b_{old} \right] + y\left[ \iprod{\boldsymbol{x} ,y \boldsymbol{x}} + y \right] \\
        &amp;= y\left[ \iprod{\boldsymbol{x}, \boldsymbol{w_{old}}} + b_{old} \right] + y\left[ y \llnorm{\boldsymbol{x}}^2 + y \right] \\
        &amp;= y\left[ \iprod{\boldsymbol{x}, \boldsymbol{w_{old}}} + b_{old} \right] + y^2 \llnorm{\boldsymbol{x}}^2 + y^2 \\
        &amp;= y\left[ \iprod{\boldsymbol{x}, \boldsymbol{w_{old}}} + b_{old} \right] + \underbrace{\llnorm{\boldsymbol{x}}^2 + 1}_{\text{always positive}}
    \end{split}
\end{equation}
Notice that $y \in \left\{ \pm 1 \right\} \Rightarrow y^2 = 1$.

$\llnorm{\boldsymbol{x}}^2 + 1$ is always positive, which means we always increase the confidence $y \hat{y}$ after the update.
\end{question}

\begin{question}[Trick for Hiding the Bias Term -- Padding]
    \begin{equation}
        \begin{split}
            \iprod{\boldsymbol{x}, \boldsymbol{w}} + b 
            &amp; = \iprod{ \begin{pmatrix} \boldsymbol{x} \\ 1 \end{pmatrix}, \begin{pmatrix} \boldsymbol{w} \\ b \end{pmatrix} } \\
            &amp; = \iprod{ \boldsymbol{x_{pad}}, \boldsymbol{w_{pad}}}
        \end{split}
    \end{equation}
    Correspondingly, the update rule can be written as:
    \begin{equation}
        \boldsymbol{w_{pad}} \leftarrow \boldsymbol{w_{pad}} + y \boldsymbol{x_{pad}}
    \end{equation}
\end{question}

\begin{question}[Margin]
    Suppose $\exists \boldsymbol{w^*}$ such that $\forall i, y_i \iprod{\boldsymbol{x_i}, \boldsymbol{w^*}} &gt; 0$.

    We normalize $\boldsymbol{w^*}$ such that $\llnorm{\boldsymbol{w^*}} = 1$.

    In other words, $\boldsymbol{w^*}$ is the normalized weight for the deicision boundary.

    \begin{equation} \label{margin}
        \text{Margin } \gamma \coloneq \min_i \left| \iprod{\boldsymbol{x_i}, \boldsymbol{w^*}} \right|
    \end{equation}
\end{question}

\begin{question}[Convergence Theorem -- Linearly Separable Case]
    Assume that $\forall i, \llnorm{\boldsymbol{x_i}} \leq C$ (i.e. within a circle with radius $C$).

    Then the Perceptron algorithm converges after $\frac{C^2}{\gamma^2}$ mistakes.

    \vspace{5mm}
    \textbf{Proof:}

    Suppose $\boldsymbol{w}$ is the updating weight, and $\theta$ is the angle between $\boldsymbol{w}$ and $\boldsymbol{w^*}$. 

    We have $\iprod{\boldsymbol{w}, \boldsymbol{w^*}} = \llnorm{\boldsymbol{w}} \llnorm{\boldsymbol{w^*}} \cos{\theta} = \llnorm{\boldsymbol{w}} \cos{\theta}$.
    
    After an update, $\llnorm{\boldsymbol{w_{new}}} \cos{\theta_{new}}$ will be
    \begin{equation}
        \begin{split}
            \iprod{\boldsymbol{w} + y \boldsymbol{x}, \boldsymbol{w^*}}
            &amp; = \iprod{\boldsymbol{w}, \boldsymbol{w^*}} + y \iprod{\boldsymbol{x}, \boldsymbol{w^*}} \\
            &amp; = \iprod{\boldsymbol{w}, \boldsymbol{w^*}} + \left|\iprod{\boldsymbol{x}, \boldsymbol{w^*}}\right| \\
            &amp; \geq \iprod{\boldsymbol{w}, \boldsymbol{w^*}} + \gamma
        \end{split}
    \end{equation}

    Let's see the change of $\iprod{\boldsymbol{w_{new}}, \boldsymbol{w_{new}}} = \llnorm{\boldsymbol{w_{new}}}^2$,
    \begin{equation}
        \begin{split}
            \iprod{\boldsymbol{w} + y \boldsymbol{x}, \boldsymbol{w} + y \boldsymbol{x}}
            &amp; = \iprod{\boldsymbol{w}, \boldsymbol{w}} + 2y \iprod{\boldsymbol{w}, \boldsymbol{x}} + y^2 \iprod{\boldsymbol{x}, \boldsymbol{x}} \\
            &amp; = \llnorm{\boldsymbol{w}}^2 + 2y \iprod{\boldsymbol{w}, \boldsymbol{x}} + \llnorm{\boldsymbol{x}}^2
        \end{split}
    \end{equation}
    Because $y\iprod{\boldsymbol{w}, \boldsymbol{x}} &lt; 0$ and $\llnorm{\boldsymbol{x}} \leq C$,
    \begin{equation}
        \begin{split}
            \iprod{\boldsymbol{w} + y \boldsymbol{x}, \boldsymbol{w} + y \boldsymbol{x}}
            &amp; = \llnorm{\boldsymbol{w}}^2 + 2y \iprod{\boldsymbol{w}, \boldsymbol{x}} + \llnorm{\boldsymbol{x}}^2 \\
            &amp; \leq \llnorm{\boldsymbol{w}}^2 + C^2
        \end{split}
    \end{equation}

    Finally, suppose it converges after $M$ updates, we have $\iprod{\boldsymbol{w}, \boldsymbol{w^*}} \geq M \gamma$ and $\llnorm{\boldsymbol{w}}^2 \leq M C^2$
    \begin{equation}
        \begin{split}
            1 = \cos{\theta} &amp; = \frac{\iprod{\boldsymbol{w}, \boldsymbol{w^*}}}{\llnorm{\boldsymbol{w}} \llnorm{\boldsymbol{w^*}}} \\
            &amp; \geq \frac{M \gamma}{\sqrt{M C^2} \times 1} \\
            &amp; = \sqrt{M} \frac{\gamma}{C}
        \end{split}
    \end{equation}

    which means $M \leq \frac{C^2}{\gamma^2}$.
\end{question}

\begin{question}[Perceptron Loss]
    \begin{equation}
        \begin{split}
            l(\boldsymbol{w}, \boldsymbol{x_t}, y_t) 
            &amp; = -y_t \iprod{\boldsymbol{w}, \boldsymbol{x_t}} \mathbb{I}[\text{mistake on }\boldsymbol{x_t}] \\
            &amp; = -\min \left\{ y_t \iprod{\boldsymbol{w}, \boldsymbol{x_i}}, 0 \right\}
        \end{split}
    \end{equation}
    \begin{equation}
        L(\boldsymbol{w}) = -\frac{1}{n} \sum_{t=1}^n y_t \iprod{\boldsymbol{w}, \boldsymbol{x_t}} \mathbb{I} [\text{mistake on }\boldsymbol{x_t}]
    \end{equation}
\end{question}

\section{Linear Regression}

\begin{question}[Least Square Regression]
   \begin{equation}
    \min_{f: \mathcal{X} \rightarrow \mathcal{Y}} \mathbb{E} \llnorm{f(\boldsymbol{X}) - Y}^2
   \end{equation} 

   The optimal regression function is
   \begin{equation}
    f^*(\boldsymbol{x}) = m(x) = \mathbb{E} [Y | \boldsymbol{X} = \boldsymbol{x}]
   \end{equation}

   Calculating it needs to know the distribution, i.e., all pairs $(\boldsymbol{X}, Y)$.
\end{question}

\begin{question}[Bias-Variance Decomposition]
    \begin{equation}
        \begin{split}
            \mathbb{E} \llnorm{f(\boldsymbol{X}) - Y}^2
            &amp; = \mathbb{E} \llnorm{f(\boldsymbol{X}) - m(x) + m(x) - Y}^2 \\
            &amp; = \mathbb{E} \llnorm{f(\boldsymbol{X}) - m(x)}^2 + \mathbb{E} \llnorm{m(x) - Y}^2 + 2 \mathbb{E} \iprod{f(\boldsymbol{X}) - m(x), m(x) - Y} \\
            &amp; = \mathbb{E} \llnorm{f(\boldsymbol{X}) - m(x)}^2 + \mathbb{E} \llnorm{m(x) - Y}^2 + \mathbb{E} \; \mathbb{E}_{Y|\boldsymbol{X}} \left[ \iprod{f(\boldsymbol{X}) - m(x), m(x) - Y} \right] \\
            &amp; = \mathbb{E} \llnorm{f(\boldsymbol{X}) - m(x)}^2 + \mathbb{E} \llnorm{m(x) - Y}^2 + \mathbb{E} \iprod{f(\boldsymbol{X}) - m(x), m(x) - \mathbb{E}_{Y|\boldsymbol{X}} [Y]} \\
            &amp; = \mathbb{E} \llnorm{f(\boldsymbol{X}) - m(x)}^2 + \mathbb{E} \llnorm{m(x) - Y}^2 + \mathbb{E} \iprod{f(\boldsymbol{X}) - m(x), m(x) - m(x)} \\
            &amp; = \mathbb{E} \llnorm{f(\boldsymbol{X}) - m(x)}^2 + \underbrace{\mathbb{E} \llnorm{m(x) - Y}^2}_{\text{noise (variance)}}
        \end{split}
    \end{equation} 

    The last term is the noise (variance), irrelevant to $f$. 
    So, to minimize the squared error, we need $f \approx m$.

    However, $m(\boldsymbol{x})$ is incalculable, because $\mathbb{E} [Y | \boldsymbol{X} = \boldsymbol{x}]$ is unknown.

    Let's learn $f_D$ from the training data $D$. Define $\bar{f}(\boldsymbol{X}) = \mathbb{E}_D[f_D(\boldsymbol{X})]$.
    \begin{equation}
        \begin{split}
            \underbrace{\mathbb{E}_{\boldsymbol{X}, Y, D} \llnorm{f_D(\boldsymbol{X}) - Y}^2}_{\text{test error}}
            &amp; = \mathbb{E}_{\boldsymbol{X}} \llnorm{f_D(\boldsymbol{X}) - m(x)}^2 + \underline{\mathbb{E}_{\boldsymbol{X}, Y} \llnorm{m(x) - Y}^2} \\
            &amp; = \mathbb{E}_{\boldsymbol{X}, D} \llnorm{f_D(\boldsymbol{X}) - \bar{f}(\boldsymbol{X}) + \bar{f}(\boldsymbol{X}) - m(x)}^2 + \underline{\mathbb{E}_{\boldsymbol{X}, Y} \llnorm{m(x) - Y}^2} \\
            &amp; = \mathbb{E}_{\boldsymbol{X}, D} \llnorm{f_D(\boldsymbol{X}) - \bar{f}(\boldsymbol{X})}^2 + \mathbb{E}_{\boldsymbol{X}} \llnorm{\bar{f}(\boldsymbol{X}) - m(x)}^2 \\
               &amp; \qquad + 2 \mathbb{E}_{\boldsymbol{X}, D} \iprod{f_D(\boldsymbol{X}) - \bar{f}(\boldsymbol{X}), \bar{f}(\boldsymbol{X}) - m(x)} \\
               &amp; \qquad + \underline{\mathbb{E}_{\boldsymbol{X}, Y} \llnorm{m(x) - Y}^2} \\
            &amp; = \dots + 2 \mathbb{E}_{\boldsymbol{X}} \mathbb{E}_{D} \iprod{f_D(\boldsymbol{X}) - \bar{f}(\boldsymbol{X}), \bar{f}(\boldsymbol{X}) - m(x)} + \underline{\dots} \\
            &amp; = \dots + 2 \mathbb{E}_{\boldsymbol{X}} \iprod{\mathbb{E}_{D} [f_D(\boldsymbol{X})] - \bar{f}(\boldsymbol{X}), \bar{f}(\boldsymbol{X}) - m(x)} + \underline{\dots} \\
            &amp; = \dots + 2 \mathbb{E}_{\boldsymbol{X}} \iprod{\bar{f}(\boldsymbol{X}) - \bar{f}(\boldsymbol{X}), \bar{f}(\boldsymbol{X}) - m(x)} + \underline{\dots} \\
            &amp; = \dots + 0 + \underline{\dots} \\
            &amp; = \mathbb{E}_{\boldsymbol{X}, D} \llnorm{f_D(\boldsymbol{X}) - \bar{f}(\boldsymbol{X})}^2 + \mathbb{E}_{\boldsymbol{X}} \llnorm{\bar{f}(\boldsymbol{X}) - m(x)}^2 + \underline{\mathbb{E}_{\boldsymbol{X}, Y} \llnorm{m(x) - Y}^2} \\
            &amp; = \underbrace{\mathbb{E}_{\boldsymbol{X}, D} \llnorm{f_D(\boldsymbol{X}) - \mathbb{E}_D [f_D(\boldsymbol{X})]}^2}_{\text{variance}} + \underbrace{\mathbb{E}_{\boldsymbol{X}} \llnorm{\mathbb{E}_D [f_D(\boldsymbol{X})] - m(x)}^2}_{\text{bias}^2} + \underbrace{\mathbb{E}_{\boldsymbol{X}, Y} \llnorm{m(x) - Y}^2}_{\text{noise (variance)}} \\
        \end{split}
    \end{equation}
\end{question}

\begin{question}[Sampling $\rightarrow$ Training]
    Replace expectation with sample average: $\paren{\boldsymbol{X_i}, Y_i} \tilde P$.
    \begin{equation}
        \min_{f: \mathcal{X} \rightarrow \mathcal{Y}} \hat{\mathbb{E}} \llnorm{f(\boldsymbol{X}) - Y}^2 \coloneq \frac{1}{n} \sum_{i=1}^n \llnorm{f(\boldsymbol{X_i}) - Y_i}^2
    \end{equation}
    Uniform law of large numbers: as training data size $n \rightarrow \argmin \mathbb{E}$, 

    $\hat{\mathbb{E}} \rightarrow \mathbb{E}$ and (hopefully) $\argmin \hat{\mathbb{E}} \rightarrow \mathbb{E}$.
\end{question}

\begin{question}[Linear Regression]
    Padding: $\boldsymbol{x} \leftarrow \begin{pmatrix}\boldsymbol{x} \\ 1\end{pmatrix}$, $W \leftarrow \begin{pmatrix}W \\ \boldsymbol{b}\end{pmatrix}$
    \begin{align*}
    X &amp; = [\boldsymbol{x_1}, \dots, \boldsymbol{x_n}] \in \mathbb{R}^{(d+1) \times n}, \\
    Y &amp; = [\boldsymbol{y_1}, \dots, \boldsymbol{y_n}] \in \mathbb{R}^{t \times n}, \\
    W &amp; \in \mathbb{R}^{t \times (d+1)}, \\
    \fnorm{A} &amp; = \sqrt{\sum_{ij} a_{ij}^2}
    \end{align*}
    Linear regression is:
    \begin{equation}
        \min_{W \in \mathbb{R}^{t \times (d+1)}} \frac{1}{n} \fnorm{WX - Y}^2
    \end{equation}
\end{question}

\begin{question}[Optimality Condition]
    If $\boldsymbol{w}$ is a minimizer (or maximizer) of a differentiable function $f$
    \textbf{over an open set}, then $f'(\boldsymbol{w}) = 0$.
\end{question}

\begin{question}[Solving Linear Regression]
    \begin{equation}
        L(W) = \frac{1}{n} \fnorm{WX-Y}^2
    \end{equation}
    \begin{equation}
        \begin{split}
            \nabla_W L(W) &amp; = \frac{2}{n} (WX-Y) X^T = 0 \\
            &amp; \Rightarrow W X X^T = Y X^T \\
            &amp; \Rightarrow W = Y X^T (X X^T)^{-1} \\
        \end{split}
    \end{equation}
\end{question}

\begin{question}[Ill-Conditioning]
    Slight pertubation leads to chaotic behavior, which happens whenever
    $X$ is ill-conditioned, i.e., (close to) rank-deficient.

    \vspace{5mm}
    Rank-deficient X means:
    \begin{enumerate}
        \item two columns in $X$ are linearly dependent (or simply the same)
        \item but the corresponding $y$ might be different
    \end{enumerate}
\end{question}

\begin{question}[Ridge Regression]
    \begin{equation}
        \min_{W} \frac{1}{n} \fnorm{WX - Y}^2 + \lambda \fnorm{W}^2
    \end{equation}
    \begin{equation}
        \begin{split}
            \nabla_W L(W) &amp; = \frac{2}{n} (WX - Y) X^T + 2 \lambda W = 0 \\
            &amp; \Rightarrow W X X^T - Y X^T + \lambda W = 0 \\
            &amp; \Rightarrow W (X X^T + n \lambda I) = Y X^T
        \end{split}
    \end{equation}
    \begin{equation}
        \begin{split}
            &amp; X = U \Sigma V^T \\
            \Rightarrow &amp; X X^T = U \Sigma (V^T V) \Sigma U^T = U \Sigma^2 U^T \\
            \Rightarrow &amp; X X^T + n \lambda I = U \underbrace{(\Sigma^2 + n \lambda I)}_{\text{strictly positive}} U^T \\
            \Rightarrow &amp; X X^T + n \lambda I \text{ is of full-rank}
        \end{split}
    \end{equation}

    $\lambda$ is regularization parameter. $\lambda = \infty \Rightarrow W \equiv \boldsymbol{0}$.
\end{question}

\begin{question}[Regularization $\equiv$ Data Augmentation]
    \begin{equation}
        \frac{1}{n} \fnorm{WX - Y}^2 + \lambda \fnorm{W}^2
        = \frac{1}{n} \fnorm{W \left[X \quad \sqrt{n\lambda} I \right] - \left[Y \quad \boldsymbol{0}\right]}^2
    \end{equation}
\end{question}

\section{Logistic Regression}

\begin{question}[Max Likelihood Estimation]
    Let $\mathcal{Y} = \{0, 1\}$. 
    Learn confidence $p(\boldsymbol{x}; \boldsymbol{w}) \coloneq \Pr (Y=1|X=\boldsymbol{x})$.

    \begin{equation}
        \begin{split}
            \max_{\boldsymbol{w}} \Pr (Y_1 = y_1, \dots, Y_n = y_n)
            &amp; = \max_{\boldsymbol{w}} \prod_{i=1}^n \Pr (Y_i = y_i | X_i = x_i) \\
            &amp; \stackrel{\mathcal{Y}=\{0, 1\}}{=} \max_{\boldsymbol{w}} \prod_{i=1}^n \left[p(\boldsymbol{x_i}; \boldsymbol{w})\right]^{y_i} \left[1 - p(\boldsymbol{x_i}; \boldsymbol{w})\right]^{1 - y_i} \\
        \end{split}
    \end{equation}

    Use negative log-likelihood:

    \begin{equation}
        \min_{\boldsymbol{w}} \sum_{i=1}^n \left[ -y_i \log p(\boldsymbol{x_i}; \boldsymbol{w}) - (1 - y_i) \log (1 - p(\boldsymbol{x_i}; \boldsymbol{w})) \right]
    \end{equation}
\end{question}

\begin{question}[Odds Ratio and Sigmoid]
    \begin{equation}
        \text{Odds Ratio} = \frac{\Pr}{1 - \Pr}
    \end{equation}

    Assume $\log \frac{p(\boldsymbol{x};\boldsymbol{w})}{1 - p(\boldsymbol{x};\boldsymbol{w})} = \iprod{\boldsymbol{x}, \boldsymbol{w}}$.

    \vspace{5mm}
    The Sigmoid transformation is:
    \begin{equation}
        p(\boldsymbol{x}; \boldsymbol{w}) = \frac{1}{1 + \exp (-\iprod{\boldsymbol{x}, \boldsymbol{w}})}
    \end{equation}
\end{question}

\begin{question}[Logistic Regression]
    Plug the sigmoid in the negative log-likelihood:
    \begin{equation}
        \begin{split}
        &amp; \min_{\boldsymbol{w}} \sum_{i=1}^n \left[ -y_i \log p(\boldsymbol{x_i}; \boldsymbol{w}) - (1 - y_i) \log (1 - p(\boldsymbol{x_i}; \boldsymbol{w})) \right] \\
        = &amp; \min_{\boldsymbol{w}} \sum_{i=1}^n \left[ y_i \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})] - (1 - y_i) \log \left(1 - \frac{1}{1 + \exp (-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})}\right) \right] \\
        = &amp; \min_{\boldsymbol{w}} \sum_{i=1}^n \left[ y_i \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})] - (1 - y_i) \log \left(\frac{\exp (-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})}{1 + \exp (-\iprod{\boldsymbol{x}, \boldsymbol{w}})}\right) \right] \\
        = &amp; \min_{\boldsymbol{w}} \sum_{i=1}^n \left[ y_i \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})] + (1 - y_i) \left[\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + \log(1 + \exp (-\iprod{\boldsymbol{x}, \boldsymbol{w}}))\right] \right] \\
        = &amp; \min_{\boldsymbol{w}} \sum_{i=1}^n \left[ \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})] + (1 - y_i) \iprod{\boldsymbol{x_i}, \boldsymbol{w}} \right] \\
        \end{split}
    \end{equation}
    Because $y_i \in \{0, 1\}$, let's map it to $\{\pm 1\}$.
    \begin{equation}
        \begin{split}
            L(\boldsymbol{w})
            &amp; \stackrel{y_i \in \{0, 1\}}{=} \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})] + (1 - y_i) \iprod{\boldsymbol{x_i}, \boldsymbol{w}} \\
            &amp; \stackrel{y_i \in \{0, 1\}}{=} \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})] + \log \left[ \exp ( (1 - y_i) \iprod{\boldsymbol{x_i}, \boldsymbol{w}} ) \right] \\
            &amp; \stackrel{y_i \in \{0, 1\}}{=} \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}}) \cdot \exp ( (1 - y_i) \iprod{\boldsymbol{x_i}, \boldsymbol{w}} ) ] \\
            &amp; \stackrel{y_i \in \{0, 1\}}{=} \log [\exp((1-y_i)\iprod{\boldsymbol{x_i}, \boldsymbol{w}}) + \exp(-y_i \iprod{\boldsymbol{x_i}, \boldsymbol{w}})] \\
            &amp; \stackrel{y_i \in \{0, 1\}}{=} \begin{cases}
                \log [\exp(\iprod{\boldsymbol{x_i}, \boldsymbol{w}}) + 1] &amp; y_i = 0 \\
                \log [1 + \exp(-\iprod{\boldsymbol{x_i}, \boldsymbol{w}})] &amp; y_i = 1
            \end{cases} \\
            &amp; \stackrel{y_i \in \{\pm 1\}}{=} \log [ 1 + \exp(-y_i \iprod{\boldsymbol{x_i}, \boldsymbol{w}}) ]
        \end{split}
    \end{equation}
\end{question}

\begin{question}[Multi-Class: Sigmoid $\rightarrow$ Softmax]
    \begin{equation}
        \Pr(Y=k | X = \boldsymbol{x}; \boldsymbol{W} = [\boldsymbol{w_1}, \dots, \boldsymbol{w_c}])
        = \frac{\exp(\iprod{\boldsymbol{x}, \boldsymbol{w_k}})}{\sum_{l=1}^c \exp(\iprod{\boldsymbol{x}, \boldsymbol{w_l}})}
    \end{equation}
    Maximum likelihood estimation (log loss, cross-entropy loss):
    \begin{equation}
        \min_{\boldsymbol{W}} \sum_{i=1}^n \left[ -\log \frac{\exp(\iprod{\boldsymbol{x}, \boldsymbol{w_k}})}{\sum_{l=1}^c \exp(\iprod{\boldsymbol{x}, \boldsymbol{w_l}})} \right]
    \end{equation}
\end{question}

\section{Hard-Margin Support Vector Machines}

\begin{question}[Distance from a Point to a Hyperplane]
    Let $H \coloneq \{\boldsymbol{x}: \iprod{\boldsymbol{x}, \boldsymbol{w}} + b = 0\}$, $\boldsymbol{x}$ be any vector in $H$.
    \begin{equation}
        \text{Distance}(\boldsymbol{x_i}, \boldsymbol{w})
        = \frac{|\iprod{\boldsymbol{x_i} - \boldsymbol{x}, \boldsymbol{w}}|}{\llnorm{\boldsymbol{w}}}
        = \frac{|\iprod{\boldsymbol{x_i}, \boldsymbol{w}} - \iprod{\boldsymbol{x}, \boldsymbol{w}}|}{\llnorm{\boldsymbol{w}}}
        \stackrel{\boldsymbol{x} \in H}{=} \frac{|\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b|}{\llnorm{\boldsymbol{w}}}
        \stackrel{y_i \hat{y_i} &gt; 0}{=} \frac{y_i \hat{y_i}}{\llnorm{\boldsymbol{w}}}
    \end{equation}
\end{question}

\begin{question}[Margin Maximization]
    Margin is the smallest distance to $H$ among all separable data.
    \begin{equation}
        \max_{\boldsymbol{w}, b} \min_i \frac{y_i \hat{y_i}}{\llnorm{\boldsymbol{w}}} 
        \text{, such that }
        \forall i, y_i \hat{y_i} &gt; 0
    \end{equation}
    Let $c &gt; 0$, then $\boldsymbol{w} = c\boldsymbol{w}, b = cb$ keeps the loss same:
    \begin{equation}
        \begin{split}
            \max_{\boldsymbol{w}, b} \min_i \frac{c y_i \hat{y_i}}{\llnorm{c\boldsymbol{w}}} 
            &amp; = \max_{\boldsymbol{w}, b} \min_i \frac{y_i (\iprod{\boldsymbol{x}, c\boldsymbol{w}} + cb)}{\llnorm{c\boldsymbol{w}}} \\
            &amp; = \max_{\boldsymbol{w}, b} \min_i \frac{c y_i (\iprod{\boldsymbol{x}, \boldsymbol{w}} + b)}{c \llnorm{\boldsymbol{w}}} \\
            &amp; = \max_{\boldsymbol{w}, b} \min_i \frac{y_i (\iprod{\boldsymbol{x}, \boldsymbol{w}} + b)}{\llnorm{\boldsymbol{w}}} \\
            &amp; = \max_{\boldsymbol{w}, b} \min_i \frac{y_i \hat{y_i}}{\llnorm{\boldsymbol{w}}} 
        \end{split}
    \end{equation}
    Let $c = \frac{1}{\min_i y_i \hat{y_i}}$,
    \begin{equation}
        \begin{split}
            \max_{\boldsymbol{w}, b} \min_i \frac{c y_i \hat{y_i}}{c \llnorm{\boldsymbol{w}}} 
            &amp; = \max_{\boldsymbol{w}, b} \frac{1}{c \llnorm{\boldsymbol{w}}} \\
            &amp; = \max_{\boldsymbol{w}, b} \frac{1}{\llnorm{\boldsymbol{w}}} \text{ s.t. } \min_i y_i \hat{y_i} = 1
        \end{split}
    \end{equation}
    Max $\rightarrow$ Min:
    \begin{equation}
        \min_{\boldsymbol{w}, b} \frac{1}{2} \llnorm{\boldsymbol{w}}^2 \text{ s.t. } \forall i, y_i \hat{y_i} \geq 1
    \end{equation}
\end{question}

\begin{question}[Hard-Margin SVM v.s. Perceptron]
    \begin{align}
        \text{Hard-Margin SVM:} \qquad
        &amp; \min_{\boldsymbol{w}, b} \frac{1}{2} \llnorm{\boldsymbol{w}}^2 &amp; \text{ s.t. } \forall i, y_i \hat{y_i} \geq 1 \\
        \text{Perceptron:} \qquad
        &amp; \min_{\boldsymbol{w}, b} 0 &amp; \text{ s.t. } \forall i, y_i \hat{y_i} \geq 1
    \end{align}
\end{question}

\begin{question}[Lagrangian Dual]
    Dual variables $\boldsymbol{\alpha} \in \mathbb{R}^n$.
    \begin{equation}
        \begin{split}
            \min_{\boldsymbol{w}, b} \; \max_{\boldsymbol{\alpha} \geq 0} \; \frac{1}{2} \llnorm{\boldsymbol{w}}^2 - \sum_i \alpha_i \left[ y_i (\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b) - 1 \right] 
            &amp; = \min_{\boldsymbol{w}, b} \; \begin{cases}
                + \infty, &amp; \text{if } \exists i, y_i (\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b) &lt; 1 \quad (\alpha_i = + \infty) \\
                \frac{1}{2} \llnorm{\boldsymbol{w}}^2 , &amp; \text{if } \forall i, y_i (\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b) \geq 1 \quad (\forall i, \alpha_i = 0) \\
            \end{cases} \\
            &amp; = \min_{\boldsymbol{w}, b} \; \frac{1}{2} \llnorm{\boldsymbol{w}}^2, \quad \text{s.t. } \forall i, y_i (\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b) \geq 1 \\
            &amp; = \min_{\boldsymbol{w}, b} \frac{1}{2} \llnorm{\boldsymbol{w}}^2 \text{ s.t. } \forall i, y_i \hat{y_i} \geq 1
        \end{split}
    \end{equation}

    Swap $\min$ and $\max$:
    \begin{equation} \label{lagrangian-max-min}
        \max_{\boldsymbol{\alpha} \geq 0} \; \min_{\boldsymbol{w}, b} \; \frac{1}{2} \llnorm{\boldsymbol{w}}^2 - \sum_i \alpha_i \left[ y_i (\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b) - 1 \right] 
    \end{equation}

    Solve inner problem by setting derivative to 0:
    \begin{equation} \label{solve-derivative-for-svm-dual}
        \frac{\delta}{\delta \boldsymbol{w}} = \boldsymbol{w} - \sum_i \alpha_i y_i \boldsymbol{x_i} = 0,
        \qquad
        \frac{\delta}{\delta b} = - \sum_i \alpha_i y_i = 0,
    \end{equation}

    Plug them into the loss:
    \begin{equation}
        \begin{split}
            L(\boldsymbol{\alpha}) 
            &amp; = \min_{\boldsymbol{w}, b} \; \frac{1}{2} \llnorm{\boldsymbol{w}}^2 - \sum_i \alpha_i \left[ y_i (\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b) - 1 \right] \\
            &amp; = \frac{1}{2} \llnorm{\sum_i \alpha_i y_i \boldsymbol{x_i}}^2 - \sum_i \alpha_i y_i \iprod{\boldsymbol{x_i}, \boldsymbol{w}} - \sum_i \alpha_i y_i b + \sum_i \alpha_i \\
            &amp; = \frac{1}{2} \llnorm{\sum_i \alpha_i y_i \boldsymbol{x_i}}^2 - \iprod{\sum_i \alpha_i y_i \boldsymbol{x_i}, \sum_i \alpha_i y_i \boldsymbol{x_i}} - b \sum_i \alpha_i y_i + \sum_i \alpha_i \\
            &amp; = \frac{1}{2} \llnorm{\sum_i \alpha_i y_i \boldsymbol{x_i}}^2 - \llnorm{\sum_i \alpha_i y_i \boldsymbol{x_i}}^2 + \sum_i \alpha_i \\
            &amp; = \sum_i \alpha_i - \frac{1}{2} \llnorm{\sum_i \alpha_i y_i \boldsymbol{x_i}}^2,
            \qquad \text{s.t. } \sum_i \alpha_i y_i = 0
        \end{split}
    \end{equation}

    So, \ref{lagrangian-max-min} is solved as:
    \begin{equation}
        \max_{\boldsymbol{\alpha} \geq 0} \sum_i \alpha_i - \frac{1}{2} \llnorm{\sum_i \alpha_i y_i \boldsymbol{x_i}}^2, 
        \qquad \text{s.t. } \sum_i \alpha_i y_i = 0
    \end{equation}

    Max $\rightarrow$ min and expand the norm:
    \begin{equation}
        \min_{\boldsymbol{\alpha} \geq 0} - \sum_i \alpha_i + \frac{1}{2} \sum_i \sum_j \alpha_i \alpha_j y_i y_j \underbrace{\iprod{\boldsymbol{x_i}, \boldsymbol{x_j}}}_{\text{Kernel, closed form w.r.t. }\boldsymbol{x_i}, \boldsymbol{x_j}}, 
        \qquad \text{s.t. } \sum_i \alpha_i y_i = 0
    \end{equation}
\end{question}

\begin{question}[Support Vectors]
    From \ref{solve-derivative-for-svm-dual}, we know $\boldsymbol{w} = \sum_i \alpha_i y_i \boldsymbol{x_i}$.

    Vectors with $\alpha_i \neq 0$ are support vectors, which lie on the margin.
\end{question}

\section{Soft-Margin Support Vector Machines}

\begin{question}[Goal]
    minimize over $\boldsymbol{w}, b$,
    \begin{equation}
        \Pr(Y \neq \sign(\hat Y)) = \Pr(Y \hat Y \leq 0)
        = \mathbb{E} \; \underbrace{\mathbb{I} [Y \hat Y \leq 0]}_{indicator function}
        \coloneq \mathbb{E} \; l_{0-1}(Y \hat Y)
    \end{equation}
    where $\hat Y = \iprod{X, \boldsymbol{w}} + b, Y = \pm 1$.

    \begin{equation}
        \begin{split}
            \min_{\hat Y: \mathcal{X} \rightarrow \mathbb{R}} \mathbb{E} \; l_{0-1}(Y \hat Y)
            &amp; = \min_{\hat Y: \mathcal{X} \rightarrow \mathbb{R}} \mathbb{E}_X \mathbb{E}_{Y|X} \; l_{0-1}(Y \hat Y) \\
            &amp; = \mathbb{E}_X \min_{\hat Y: \mathcal{X} \rightarrow \mathbb{R}} \mathbb{E}_{Y|X} \; l_{0-1}(Y \hat Y) \\
        \end{split}
    \end{equation}

    Minimizing the 0-1 error is \textbf{NP-hard}.
\end{question}

\begin{question}[Bayes Rule]
    \begin{equation}
        \eta(\boldsymbol{x}) \coloneq \argmax_{\hat y \in \mathbb{R}} \Pr (Y = \hat y | X = \boldsymbol{x})
    \end{equation}
    \begin{equation}
        \eta(\boldsymbol{x}) \coloneq \argmin_{\hat y \in \mathbb{R}} \mathbb{E}_{Y|X=\boldsymbol{x}} \; l_{0-1}(Y \hat y)
    \end{equation}
\end{question}

\begin{question}[Classification Calibrated]
    A loss $l(y \hat y)$ is classification calibrated, iff $\forall \boldsymbol{x}$,
    \begin{equation*}
    \hat y(\boldsymbol{x}) \coloneq \argmin_{\hat y \in \mathbb{R}} \mathbb{E}_{Y | X = \boldsymbol{x}} \; l(Y \hat y)
    \end{equation*}
    has the same sign as the Bayes rule $\eta(\boldsymbol{x}) \coloneq \argmin_{\hat y \in \mathbb{R}} \mathbb{E}_{Y|X=\boldsymbol{x}} \; l_{0-1}(Y \hat y)$

    Notice: $\eta(\boldsymbol{x}), \hat y (\boldsymbol{x})$ provide the \textbf{score}, their sign provides the prediction.
\end{question}

\begin{question}[Characterization under Convexity]
    Any \textbf{convex} loss $l$ is classification calibrated iff $l$ is differentiable
    at $0$ and $l'(0) &lt; 0$.
\end{question}

\begin{question}[Hinge Loss]
    \begin{equation}
        l_{hinge}(y \hat y) = (1 - y \hat y)^+ \coloneq \max \{0, 1 - y \hat y\}
        = \begin{cases}
            1 - y \hat y, &amp; \text{if } y \hat y &lt; 1 \\
            0, &amp; \text{otherwise}
        \end{cases}
    \end{equation}
    The classifier that minimizes the expected hinge loss minimizes the expected 0-1 loss.
\end{question}

\begin{question}[Soft-Margin SVM]
    \begin{equation}
        \min_{\boldsymbol{w}, b} \frac{1}{2} \llnorm{\boldsymbol{w}}^2 + C \cdot \sum_i l_{hinge}(y_i \hat y_i), \quad \text{s.t. } \hat y_i = \iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b
    \end{equation}
\end{question}
    

\begin{question}[Lagrangian Dual]
    Apply $C \cdot l_{hinge}(t) \coloneq \max\{0, C(1 - t)\} = \max_{0 \leq \alpha \leq C} \alpha (1 - t)$
    \begin{equation}
        \min_{\boldsymbol{w}, b} \max_{0 \leq \boldsymbol{\alpha} \leq C} \frac{1}{2} \llnorm{\boldsymbol{w}}^2 + \sum_i \alpha_i (1 - y_i \hat y_i)
    \end{equation}

    Swap $\min$ with $\max$:
    \begin{equation}
        \max_{0 \leq \boldsymbol{\alpha} \leq C} \min_{\boldsymbol{w}, b} \frac{1}{2} \llnorm{\boldsymbol{w}}^2 + \sum_i \alpha_i (1 - y_i \hat y_i)
    \end{equation}

    Solve it by setting derivative to 0:
    \begin{equation} 
        \frac{\delta}{\delta \boldsymbol{w}} = \boldsymbol{w} - \sum_i \alpha_i y_i \boldsymbol{x_i} = 0,
        \qquad
        \frac{\delta}{\delta b} = - \sum_i \alpha_i y_i = 0,
    \end{equation}

    Plug them into the loss:
    \begin{equation}
        \begin{split}
            \max_{0 \leq \boldsymbol{\alpha} \leq C} \min_{\boldsymbol{w}, b} \frac{1}{2} \llnorm{\boldsymbol{w}}^2 + \sum_i \alpha_i (1 - y_i \hat y_i)
            &amp; = \max_{0 \leq \boldsymbol{\alpha} \leq C} \min_{\boldsymbol{w}, b} \; \frac{1}{2} \llnorm{\boldsymbol{w}}^2 + \sum_i \alpha_i \left[ 1 - y_i (\iprod{\boldsymbol{x_i}, \boldsymbol{w}} + b) \right] \\
            &amp; = \max_{0 \leq \boldsymbol{\alpha} \leq C} \sum_i \alpha_i - \frac{1}{2} \llnorm{\sum_i \alpha_i y_i \boldsymbol{x_i}}^2,
            \qquad \text{s.t. } \sum_i \alpha_i y_i = 0
        \end{split}
    \end{equation}

    Max $\rightarrow$ min and expand the norm:
    \begin{equation}
        \min_{0 \leq \boldsymbol{\alpha} \leq C} - \sum_i \alpha_i + \frac{1}{2} \sum_i \sum_j \alpha_i \alpha_j y_i y_j \underbrace{\iprod{\boldsymbol{x_i}, \boldsymbol{x_j}}}_{\text{Kernel, closed form w.r.t. }\boldsymbol{x_i}, \boldsymbol{x_j}}, 
        \qquad \text{s.t. } \sum_i \alpha_i y_i = 0
    \end{equation}

    $C \rightarrow \infty \Rightarrow \text{Hard-margin SVM}$,
    $C \rightarrow 0 \Rightarrow \text{a constant classifier}$
\end{question}

\section{Reproducing Kernels}

\begin{question}[(Reproducing) Kernels]
    $k: \mathcal(X) \times \mathcal{X} \rightarrow \mathbb{R}$ is a (reproducing) kernel
    iff there exists some $\Phi: \mathcal{X} \rightarrow \mathcal{H}$ so that
    $\iprod{\Phi(\boldsymbol{x}), \Phi(\boldsymbol{z})} = k(\boldsymbol{x}, \boldsymbol{z})$.
    
    \begin{itemize}
        \item A feature transform $\Phi$ determines the corresponding kernel $k$.
        \item A kernel $k$ determines some feature transforms $\Phi$, but may not be unique. \\
        E.g. $\iprod{\phi(\boldsymbol{x}), \phi(\boldsymbol{z})} = \iprod{\phi'(\boldsymbol{x}), \phi'(\boldsymbol{z})}$
        \begin{enumerate}
            \item $\phi(x) \coloneq [x_1^2, \sqrt{2} x_1 x_2] \in \mathbb{R}^2$
            \item $\phi'(x) \coloneq [x_1^2, x_1 x_2, x_1 x_2] \in \mathbb{R}^3$
        \end{enumerate}
    \end{itemize}
\end{question}

\begin{question}[Mercer's Theorem]
    $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is a kernel, \\
    iff $\forall n \in \mathbb{N}, \forall \boldsymbol{x_1}, \dots, \boldsymbol{x_n} \in \mathcal{X}$,
    the kernel matrix $K$ such that $K_{ij} \coloneq k(\boldsymbol{x_i}, \boldsymbol{x_j})$ is symmetric and positive semi-definite (PSD).

    \begin{equation}
        k \text{ is a kernel} \Leftrightarrow \begin{cases}
            K_{ij} = K_{ji} &amp; \text{(symmetric)}\\
            \iprod{\boldsymbol{\alpha}, K \boldsymbol{\alpha}} = \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j K_{ij} \geq 0 \qquad \forall \boldsymbol{\alpha} \in \mathbb{R}^n &amp; \text{(PSD)}
        \end{cases}
    \end{equation}
\end{question}

\begin{question}[Symmetric PSD]
    For a symmetric matrix $A$, the following conditions are equivalent.
    \begin{enumerate}
        \item $A \succeq 0$
        \item $A = U^T U$ for some matrix $U$
        \item $x^T A x \geq 0$ for every $x \in \mathbb{R}^n$
        \item All principal minors of $A$ are nonnegative
    \end{enumerate}
\end{question}

\section{Gradient Descent}

\begin{question}[Gradient Descent Template]
    Choose initial point $x^{(0)} \in \mathbb{R}^d$ and repeat:
    \begin{equation}
        x^{(k)} = x^{(k-1)} - \underbrace{\eta}_{\text{step size}} \nabla f(x^{(k-1)}), \quad k = 1, 2, \dots
    \end{equation}
\end{question}

\begin{question}[Interpretation from Taylor Expansion]
    Expand $f$ locally at $x$:
    \begin{equation}
        \begin{split}
            f(y) &amp; \approx f(x) + \nabla f(x)^T(y-x) + \frac{1}{2t}\llnorm{y-x}^2 \\
            \Rightarrow \min_y f(y) &amp; \approx \min_y \left[ f(x) + \nabla f(x)^T(y-x) + \frac{1}{2t}\llnorm{y-x}^2 \right] \\
        \end{split}
    \end{equation}
    When $y - x = \frac{\nabla f(x)}{-2 \frac{1}{2t}} = -t \nabla f(x) \Longrightarrow y = x - t \nabla f(x)$, it reaches the minimum.
\end{question}

\begin{question}[$L$-smooth or $L$-Lipschitz Continuous]
    $f$ is convex and differentiable. $\nabla f$ is $L$-Lipschitz continuous ($L$-smooth):
    \begin{equation}
        L \boldsymbol{I}  \succeq \nabla^2 f(x), \forall x
    \end{equation}
\end{question}

\begin{question}[Convergence Rate for Convex Case]
    Assume $f$ is $L$-smooth.
    Gradient descent with fixed step size $t \leq \frac{1}{L}$ satisfies:
    \begin{equation}
        f(x^{(k)}) - f(x^*) \leq \frac{\llnorm{x^{(0)} - x^*}^2}{2tk}
    \end{equation}
    We say gradient descent has convergence rate $O(\frac{1}{k})$,
    i.e. $f(x^{(k)}) - f(x^*) \leq \epsilon$ can be achieved using only $O(\frac{1}{\epsilon})$ iterations.

    \vspace{5mm}
    \textbf{Proof}

    \begin{equation}
        \begin{split}
            f(y) 
            &amp; = f(x) + \nabla f(x)^T (y-x) + \frac{1}{2} (y-x)^T \nabla^2 f(\xi)(y-x) \\
            &amp; \leq f(x) + \nabla f(x)^T(y-x) + \frac{1}{2} L \llnorm{y-x}^2 \qquad (L\text{-smooth}, L \boldsymbol{I} \succeq \nabla^2 f(\xi))
        \end{split}
    \end{equation}

    Plug in gradient descent:
    \begin{equation} \label{convergence-1}
        \begin{split}
            f(x^+) 
            &amp; = f(y) \\
            &amp; \leq f(x) + \nabla f(x)^T(x - t\nabla f(x)-x) + \frac{1}{2} L \llnorm{x - t\nabla f(x)-x}^2 \\
            &amp; = f(x) -(1 - \frac{1}{2}Lt) t \llnorm{\nabla f(x)}^2 \\
            &amp; \leq f(x) - \frac{1}{2} t \llnorm{\nabla f(x)}^2 \quad (t \leq \frac{1}{L})
        \end{split}
    \end{equation}

    $f$ is convex $\Rightarrow f(x^*) \geq f(x) + \nabla f(X)^T(x^* - x) \Rightarrow f(x) \leq f(x^*) + \nabla f(x)^T(x - x^*)$

    Plug this into \ref{convergence-1}:
    \begin{equation}
        \begin{split}
            &amp; f(x^+) \leq f(x^*) + \nabla f(x)^T (x - x^*) - \frac{1}{2} t \llnorm{\nabla f(x)}^2 \\
            \Rightarrow &amp; f(x^+) - f(x^*) \leq \frac{1}{2t}\left( 2t \nabla f(x)^T(x - x^*) - t^2 \llnorm{\nabla f(x)}^2 \right) \\
            \Rightarrow &amp; f(x^+) - f(x^*) \leq \frac{1}{2t}\left( 2t \nabla f(x)^T(x - x^*) - t^2 \llnorm{\nabla f(x)}^2 - \llnorm{x-x^*}^2 + \llnorm{x-x^*}^2 \right) \\
            \Rightarrow &amp; f(x^+) - f(x^*) \leq \frac{1}{2t}\left( \llnorm{x-x^*}^2 - (\llnorm{x-x^*}^2 + t^2 \llnorm{\nabla f(x)}^2 - 2t \nabla f(x)^T(x - x^*) ) \right) \\
            \Rightarrow &amp; f(x^+) - f(x^*) \leq \frac{1}{2t}\left( \llnorm{x-x^*}^2 - \llnorm{x - x^* - t\nabla f(x)}^2 \right) \\
            \Rightarrow &amp; f(x^+) - f(x^*) \leq \frac{1}{2t}\left( \llnorm{x-x^*}^2 - \llnorm{x^+ - x^*}^2 \right) \\
        \end{split}
    \end{equation}

    Viewing $x^+$ as $x^{(i)}$ and $x$ as $x^{(x-1)}$:
    \begin{equation}
        \begin{split}
            \sum_{i=1}^k \paren{ f(x^{(i)}) - f(x^*) }
            &amp; \leq \sum_{i=1}^k \frac{1}{2t} \paren{ \llnorm{x^{(i-1)} - x^*}^2 - \llnorm{x^{(i)} - x^*}^2 } \\
            &amp; = \frac{1}{2t} \paren{ \llnorm{x^{(0)} - x^*}^2 - \llnorm{x^{(k)} - x^*}^2 } \\
            &amp; \leq \frac{1}{2t} \llnorm{x^{(0)} - x^*}^2 \\
        \end{split}
    \end{equation}

    which implies
    \begin{equation}
        f(x^{(k)}) \leq \frac{1}{k} \sum_{i=1}^k f(x^{(i)}) \leq f(x^*) + \frac{\llnorm{x^{(0)} - x^*}^2}{2tk}
    \end{equation}
\end{question}

\begin{question}[Convergence Rate for Strong Convexity]
    $f$ is differentiable, $L$-smooth, and $m$-strongly convex.

    $m$-strong convexity of $f$ means $f(x) - \frac{m}{2} \llnorm{x}^2$ is convex, i.e. $\nabla^2 f(x) \succeq m \boldsymbol{I}$

    Then, there is a constant $0 &lt; \gamma &lt; 1$ such that gradient descent with fixed step size $t \leq \frac{2}{m + L}$ satisfies:
    \begin{equation}
        f(x^{(k)}) - f(x^*) \leq \gamma^k \frac{L}{2} \llnorm{x^{(0)} - x^*}^2
    \end{equation}
    Rate is $O(\gamma^k)$. Only $O(\log_{\frac{1}{\gamma}}(\frac{1}{\epsilon}))$ iterations needed.
\end{question}

\begin{question}[Convergence Rate for Non-Convex Case]
    $f$ is differentiable and $L$-smooth, but non-convex.

    Gradient descent with fixed step size $t \leq \frac{1}{L}$ satisifes:
    \begin{equation}
        \min_{i=0,\dots,k} \llnorm{\nabla f(x^{(i)})} \leq \sqrt(\frac{2 (f(x^{(0)} - f^*))}{t(k+1)})
    \end{equation}

    Rate is $O(\frac{1}{\sqrt{k}})$ for finding stationary point.
    $O(\frac{1}{\epsilon^2})$ iterations are needed.
\end{question}

\begin{question}[Convergence Rate for Stochastic Gradient Descent]
    For convex and $L$-smooth $f$,

    \begin{itemize}
        \item Gradient Descent
        \begin{equation}
            \boldsymbol{w^+} = \boldsymbol{w} - t \cdot \frac{1}{n} \sum_{i=1}^n \nabla f_i(\boldsymbol{w})
        \end{equation}
            \begin{itemize}
                \item Step size $t \leq \frac{1}{L}$
                \item Time complexity $O(\frac{n}{\epsilon})$
            \end{itemize}
        \item Stochastic Gradient Descent
        \begin{equation}
            \boldsymbol{w^+} = \boldsymbol{w} - t \cdot \nabla f_{I_{random}}(\boldsymbol{w})
        \end{equation}
            \begin{itemize}
                \item Step size $t = \frac{1}{k}, k = 1, 2, 3, \dots$ (adaptive step size)
                \item Time complexity $O(\frac{1}{\epsilon^2})$
            \end{itemize}
    \end{itemize}
\end{question}

\section{Fully-Connected Neural Networks}

\begin{question}[Forward and Backward Pass of a 2-Layer MLP]
    A 2-layer MLP ($k$ is the NN width, $c$ is the output dim):
    \begin{align}
        \boldsymbol{x} &amp; = \text{input} &amp; (\boldsymbol{x} \in \mathbb{R}^d) \\
        \boldsymbol{z} &amp; = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b_1} &amp; (\boldsymbol{W} \in \mathbb{R}^{k \times d}, \boldsymbol{z}, \boldsymbol{b} \in \mathbb{R}^k) \\
        \boldsymbol{h} &amp; = \text{ReLU}(\boldsymbol{z}) &amp; (\boldsymbol{h} \in \mathbb{R}^k) \\
        \boldsymbol{\theta} &amp; = \boldsymbol{U} \boldsymbol{h} + \boldsymbol{b_2} &amp; (\boldsymbol{U} \in \mathbb{R}^{c \times k}, \boldsymbol{\theta}, \boldsymbol{b_2} \in \mathbb{R}^c) \\
        J &amp; = \frac{1}{2} \llnorm{\boldsymbol{\theta} - \boldsymbol{y}}^2 &amp; (\boldsymbol{y} \in \mathbb{R}^c, J \in \mathbb{R})
    \end{align}
    \begin{align}
        \text{ReLU} = \begin{cases}
            x &amp; x &gt; 0 \\
            0 &amp; x \leq 0
        \end{cases} \\
        \text{ReLU}' = \begin{cases}
            1 &amp; x &gt; 0 \\
            0 &amp; x \leq 0
        \end{cases}
    \end{align}
    
    Backward pass ($\odot$ is the Hadamard product, i.e. element-wise product):
    \begin{align}
        \frac{\delta J}{\delta \boldsymbol{\theta}} &amp; = \boldsymbol{\theta} - \boldsymbol{y} \\
        \frac{\delta J}{\delta \boldsymbol{U}} &amp; = \frac{\delta J}{\delta \boldsymbol{\theta}} \circ \frac{\delta \boldsymbol{\theta}}{\delta \boldsymbol{U}} = (\boldsymbol{\theta} - \boldsymbol{y}) \boldsymbol{h}^T \\
        \frac{\delta J}{\delta \boldsymbol{b_2}} &amp; = \frac{\delta J}{\delta \boldsymbol{\theta}} \circ \frac{\delta \boldsymbol{\theta}}{\delta \boldsymbol{b_2}} = \boldsymbol{\theta} - \boldsymbol{y} \\
        \frac{\delta J}{\delta \boldsymbol{h}} &amp; = \frac{\delta J}{\delta \boldsymbol{\theta}} \circ \frac{\delta \boldsymbol{\theta}}{\delta \boldsymbol{h}} = \boldsymbol{U^T}(\boldsymbol{\theta} - \boldsymbol{y}) \\
        \frac{\delta J}{\delta \boldsymbol{z}} &amp; = \frac{\delta J}{\delta \boldsymbol{h}} \circ \frac{\delta \boldsymbol{h}}{\delta \boldsymbol{z}} = \boldsymbol{U^T}(\boldsymbol{\theta} - \boldsymbol{y}) \odot \text{ReLU}'(\boldsymbol{z}) \\
        \frac{\delta J}{\delta \boldsymbol{W}} &amp; = \frac{\delta J}{\delta \boldsymbol{z}} \circ \frac{\delta \boldsymbol{z}}{\delta \boldsymbol{W}} = \boldsymbol{U^T}(\boldsymbol{\theta} - \boldsymbol{y}) \odot \text{ReLU}'(\boldsymbol{z}) \boldsymbol{x}^T \\
        \frac{\delta J}{\delta \boldsymbol{b_1}} &amp; = \frac{\delta J}{\delta \boldsymbol{z}} \circ \frac{\delta \boldsymbol{z}}{\delta \boldsymbol{b_1}} = \boldsymbol{U^T}(\boldsymbol{\theta} - \boldsymbol{y}) \odot \text{ReLU}'(\boldsymbol{z}) \\
    \end{align}
\end{question}

\begin{question}[Universal Approximation Theorem by 2-Layer NNs]
    For any continuous function $f: \mathbb{R}^d \rightarrow \mathbb{R}^c$
    and any $\epsilon &gt; 0$,
    there exists $k \in \mathbb{N}, \boldsymbol{W} \in \mathbb{R}^{k \times d}, \boldsymbol{b} \in \mathbb{R}^k, \boldsymbol{U} \in \mathbb{R}^{c \times k}$
    such that
    \begin{equation}
        \sup_{\boldsymbol{x}} \llnorm{f(\boldsymbol{x}) - g(\boldsymbol{x})} &lt; \epsilon
    \end{equation}
    where $g(\boldsymbol{x}) = \boldsymbol{U}(\sigma(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}))$
    and $\sigma$ is the element-wise ReLU operation.

    As long as the 2-layer MLP is wide enough, it can approximate any
    continuous function arbitrarily closely.
\end{question}

\section{Convolutional Neural Networks}

\begin{question}[Controlling the Convolution]
    Hyperparameters.

    \begin{itemize}
        \item \textbf{Filter (kernel) size}: width $times$ height.
        \item \textbf{Number of filters (kernels)}. \\
        Weights are not shared between different filters (kernels)
        \item \textbf{Stride}: how many pixels the filter moves each time.
        \item \textbf{Padding}: add zeros around the boundary of the input.
    \end{itemize}
\end{question}

\begin{question}[Size Calculation]
    \begin{align*}
        \text{Input size: } &amp; m \times n \times c_{in} \\
        \text{Filter size: } &amp; a \times b \times c_{in} \\
        \text{Stride: } &amp; s \times t \\
        \text{Padding: } &amp; p \times q
    \end{align*}

    Output size:
    \begin{equation}
        \floor*{1 + \frac{m + 2p - a}{s}} \times \floor*{1 + \frac{n + 2q - b}{t}}
    \end{equation}
\end{question}

\part{For Final}

\section{Transformer}

\begin{question}[Attention Layer Inputs and Outputs]
    Inputs: $V \in \mathcal{R}^{n \times d}$, $K \in \mathcal{R}^{n \times d}$, $Q \in \mathcal{R}^{m \times d}$,
    Outputs: an $m \times d$ matrix.

    \begin{itemize}
        \item \textbf{Self Attention}: $m = n$,
        \item \textbf{Cross Attention}: $m \neq n$ where $m$ is the sequence length of decoder, $n$ is the sequence length of encoder.
    \end{itemize}
\end{question}

\begin{question}[Attention Layer Calculation]
    \begin{equation}
        \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
    \end{equation}

    Softmax is row-wise, i.e. for each row of $QK^T$, it is normalized to sum to 1.
\end{question}

\begin{question}[Learnable Attention Layer]
    \begin{equation}
        \text{Attention}(XW^v, XW^k, XW^q) = \text{softmax}\left(\frac{XW^q(XW^k)^T}{\sqrt{d}}\right)XW^v
    \end{equation}
\end{question}

\begin{question}[RMSNorm (LLaMA's Choice)]
    \begin{equation}
        \bar{a_i} = \frac{a_i}{\text{RMS}(a)}\gamma = \frac{a_i}{\sqrt{\frac{1}{d}\sum_{j=1}^d a_j^2}}\gamma
    \end{equation}
\end{question}

\begin{question}[Transformer Loss]
    \begin{equation}
        \min_{W} \hat{\mathbb{E}}\left[ - \iprod{Y, \log \hat{Y}} \right]
    \end{equation}
    $Y$ is output sequence, one-hot; \\
    $\hat{Y}$ is the predicted probabilities
\end{question}

\begin{question}[Transformer Implementation] As following.
    \begin{lstlisting}[language=python]
import torch
import torch.nn as nn
import torch.F as F
import math

class RMSNorm(nn.Module):
  def __init__(self, hidden_dim, eps = 1e-6):
    super().__init__()
    self.eps = eps
    self.weight = nn.Parameter(torch.ones(hidden_dim))

  def forward(self, hidden_state):
    norm = hidden_state.pow(2).mean(-1, keepdim = True)
    output = hidden_state * self.weight * torch.rsqrt(norm + self.eps)
    return output

class MultiHeadAttention(nn.Module):
  def __init__(self, hidden_dim, num_heads):
    super().__init__()
    self.hidden_dim = hidden_dim
    self.num_heads = num_heads
    self.head_dim = hidden_dim // num_heads
    self.q_linear = nn.linear(hidden_dim, hidden_dim)
    self.k_linear = nn.linear(hidden_dim, hidden_dim)
    self.v_linear = nn.linear(hidden_dim, hidden_dim)
    self.o_linear = nn.linear(hidden_dim, hidden_dim)
    self.norm = RMSNorm(hidden_dim)

  def forward(self, hidden_state, mask, past_kv = None, use_cache = True):
    bs = hidden_state.shape[0]

    residual = hidden_state

    hidden_state = self.norm(hidden_state) # LLAMA style normalization

    q = self.q_linear(hidden_state) # (bs, seqlen, hidden_dim)
    k = self.k_linear(hidden_state) # (bs, seqlen, hidden_dim)
    v = self.v_linear(hidden_state) # (bs, seqlen, hidden_dim)

    q = q.view(bs, -1, self.num_heads, self.head_dim).tranpose(1, 2)
    k = k.view(bs, -1, self.num_heads, self.head_dim).tranpose(1, 2)
    v = v.view(bs, -1, self.num_heads, self.head_dim).tranpose(1, 2)
    # (bs, nums_head, seqlen, head_dim)

    q, k = apply_rope(q, k)

    # kv cache
    if past_kv is not None:
      past_k, past_v = past_kv
      k = torch.cat([past_k, k], dim = 2)
      v = torch.cat([past_v, v], dim = 2)
    new_past_kv = (k, v) if use_cache else None

    # compute attention
    attention_scores = torch.matmul(q, k.tranpose(-1, -2)) / math.sqrt(self.head_dim)
    attention_scores += mask * -1e9
    attention_scores = F.softmax(attention_scores, dim = -1)
    output = torch.matmul(attention_scores, v)

    # concat
    output = output.tranpose(1, 2).contiguous().view(bs, -1, self.hidden_dim)

    # o_linear
    output = self.o_linear(output)

    output += residual

    return output, new_past_kv if use_cache else output
    \end{lstlisting} 
\end{question}

\section{Large Language Models}

\begin{question}[BERT v.s. GPT]
    BERT is encoder; GPT is decoder.

    \begin{itemize}
        \item BERT predicts middle words; GPT predicts the next word.
        \item BERT is \textbf{NOT} auto-regressive; GPT is auto-regressive.
    \end{itemize}
\end{question}

\begin{question}[GPT -- Generative Pre-Training]
    \begin{equation}
        \min_{\Theta} \hat{\mathbb{E}} - \log \prod_{j=1}^m \Pr(x_j \vert x_1, \dots, x_{j-1}; \Theta)
    \end{equation}
\end{question}

\begin{question}[Fine-Tuning Tasks]
    Supervised fine-tuning tasks:
    \begin{equation}
        \min_{\Theta} \underbrace{- \hat{\mathbb{E}} \log \Pr(y \vert X_{1:m}; \Theta)}_{\text{task-aware supervised loss}} \underbrace{- \lambda \hat{\mathbb{E}} \log \prod_{j=1}^m \Pr(x_j \vert X_{1:j-1}; \Theta)}_{\text{pretraining loss}}
    \end{equation}
\end{question}

\begin{question}[BERT $\rightarrow$ RoBERTa]
    Training longer, with bigger batches, over more data and longer sequence.

    Removing the next sentence prediction objective.
\end{question}

\begin{question}[Sentence-BERT]
    a twin network architecture that uses BERT to derive sentence embeddings.
\end{question}

\begin{question}[GPT-2]
    1.5B parameters.

    \begin{itemize}
        \item 10x larger than GPT-1
        \item Training method is same as GPT-1.
        \item Performs on par with BERT on fine-tuning tasks.
        \item Good at zero-shot learning.
        \item Open-source.
    \end{itemize}
\end{question}

\begin{question}[GPT-3]
    175B parameters.

    \begin{itemize}
        \item 100x larger than GPT-2.
        \item Training method is same as GPT-2.
        \item New phenomenon: \textbf{in-context learning} (ICL, or few-shot learning) and \textbf{chain-of-thoughts} (CoT).
    \end{itemize}
\end{question}

\begin{question}[GPT-3.5 -- RLHF]
    Reinforcement Learning from Human Feedback (RLHF).
    \begin{itemize}
        \item state = prompt
        \item action = model output
        \item policy function = LLM
        \item reward = levels of matching human feedback
    \end{itemize}

    Pari-wise comparison loss to train reward model $r_\theta$:
    \begin{equation}
        \mathcal{L}_{\text{pair}}(\theta) = - \frac{1}{\binom{K}{2}} \mathbb{E}_{(x, y_w, y_l)} \left[ \log ( \sigma(r_\theta(x, y_w) - r_\theta(x, y_l)) ) \right]
    \end{equation}

    Proximal Policy Optimization (PPO) to maximize objective:
    \begin{equation}
        \max_{\Phi} \mathbb{E}_{(x, y)} \left[ \underbrace{r_\theta(x, y)}_{\text{maximize reward}} - \beta \underbrace{\log\left( \frac{\pi_\Phi^{\text{RL}}(y \vert x)}{\pi^{\text{SFT}}(y \vert x)}\right) }_{\text{model is close to SFT model}} + \gamma \underbrace{\mathbb{E}[\log(\pi_\Phi^{\text{RL}}(x))]}_{\text{pretraining loss}} \right]
    \end{equation}
\end{question}

\section{Speculative Sampling}

\begin{question}[Reject Sampling for Check]
    Check in parallel.

    \begin{itemize}
        \item $r \sim U(0, 1), \quad \text{if } r &lt; \underbrace{\min\left(1, \frac{p(t)}{q(t)}\right)}_{\text{accept rate}}$, next token $= t$.
        \item else: next token $= t' \sim \underbrace{\text{norm}(\max(0, p-q))}_{\text{residual distribution}}$.
    \end{itemize}
\end{question}

\begin{question}[Proof: Reject Sampling $\equiv t \sim p$]
    \begin{align}
        \begin{split}
            \min(p(t), q(t)) + \max(0, p(t) - q(t)) 
            &amp;= \begin{cases}
            p(t) + 0 &amp; \text{if } p(t) &lt; q(t) \\
            q(t) + p(t) - q(t) &amp; \text{if } p(t) \geq q(t)
            \end{cases} \\
            &amp;= p(t)
        \end{split} \\
        \Rightarrow &amp; \sum_t (\min(p(t), q(t)) + \max(0, p(t) - q(t))) = \sum_t p(t) = 1 \\
        \Rightarrow &amp; 1 - \sum_t \min(p(t), q(t)) = \sum_t \max(0, p(t) - q(t))
    \end{align}

    \begin{equation}
        \begin{split}
            \Pr(X = t) &amp;= \Pr(\tilde{X} = t) \Pr(\tilde{X}\text{ accept} \vert \tilde{X} = t) + \Pr(\tilde{X}\text{ reject}) \Pr(\tilde{\tilde{X}} = t \vert \tilde{X}\text{ reject}) \\
            &amp;= q(t) \cdot \min\left(1, \frac{p(t)}{q(t)}\right) + (1 - \Pr(\tilde{X}\text{ accept})) \cdot \text{norm}(\max(0, p(t) - q(t))) \\
            &amp;= \min\left(q(t), p(t)\right) + (1 - \sum_t \min\left(p(t), q(t)\right)) \cdot \frac{\max(0, p(t) - q(t))}{\sum_t \max(0, p(t) - q(t))} \\
            &amp;= \min\left(q(t), p(t)\right) + \max(0, p(t) - q(t)) \\
            &amp;= p(t)
        \end{split}
    \end{equation}
\end{question}

\section{Generative Adversarial Networks}

\begin{question}[Representation through Push-Forward]
    Let $r$ be any continuous distribution on $\mathbb{R}^h$.
    For any distribution $p$ on $\mathbb{R}^d$, 
    there exists push-forward maps $G: \mathbb{R}^h \rightarrow \mathbb{R}^d$ such that
    \begin{equation}
        \boldsymbol{z} \sim r \Longrightarrow G(\boldsymbol{z}) \sim p
    \end{equation}
\end{question}

\begin{question}[Discriminator's Goal]
    For a fixed generator $G$, minimize a log loss over $D$:
    \begin{itemize}
        \item If $x$ is real, minimize $-\log D(x)$;
        \item If $x$ is fake, minimize $-\log(1 - D(x))$.
    \end{itemize}

    \begin{equation}
        \min_D -\frac{1}{2} \mathbb{E}_{x \sim p_{\text{data}}} \left[\log D(x)\right] - \frac{1}{2} \mathbb{E}_{z \sim r} \left[\log(1 - D(G(z)))\right]
    \end{equation}
\end{question}

\begin{question}[Generator's Goal]
    For a fixed discriminator $D$, maximize a log loss over $G$:
    
    \begin{equation}
        \begingroup\color{red}{\max_G}\endgroup -\frac{1}{2} \mathbb{E}_{x \sim p_{\text{data}}} \left[\log D(x)\right] - \frac{1}{2} \mathbb{E}_{z \sim r} \left[\log(1 - D(G(z)))\right]
    \end{equation}
\end{question}

\begin{question}[Solver]
    \begin{equation}
        \min_G \max_D V(G, D) = \mathbb{E}_{x \sim p_{\text{data}}} \left[\log D(x)\right] + \mathbb{E}_{z \sim r} \left[\log(1 - D(G(z)))\right]
    \end{equation}

    Solved by alternative minimization-maximization:
    \begin{itemize}
        \item G step: Fix $D$ and update $G$ by one-step gradient descent
        \item D step: Fix $G$ and update $D$ by one-step gradient descent
        \item Repeat until the algorithm reaches an approximate equilibrium
    \end{itemize}
    
\end{question}

\begin{question}[Solution of $D^*$]
    Let $p_g(x)$ be the density of $x$ estimated by the generator $G$.
    For $G$ fixed, the optimal discriminator $D$ is $D_G^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}$

    \textbf{Proof:}

    \begin{equation}
        \begin{split}
            V(G, D) &amp;\coloneq \mathbb{E}_{x \sim p_{\text{data}}} \left[\log D(x)\right] + \mathbb{E}_{z \sim r} \left[\log(1 - D(G(z)))\right] \\
            &amp;= \int \log D(x) p_{\text{data}}(x) dx + \int_z \log(1 - D(G(z))) p_z(z) dz \\
            &amp;= \int \underbrace{\log D(x) p_{\text{data}}(x) + p_g(x) \log(1 - D(x))}_{f(D(x))} dx
        \end{split}
    \end{equation}

    For any fixed $x$, taking derivative $= 0$:
    \begin{equation}
        \begin{split}
            f'(D(x)) &amp;= \frac{p_{\text{data}}(x)}{D(x)} - \frac{p_g(x)}{1 - D(x)} = 0 \\
            D_G^*(x) &amp;= \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}
        \end{split}
    \end{equation}
\end{question}

\begin{question}[Solution of $G^*$]
    The global minimum of $\min_G \max_D V(G, D)$ is achieved if
    and only if $p_g = p_{\text{data}}$. The optimal objective value is $-\log4$.

    \textbf{Proof:}
    \begin{equation}
        \begin{split}
        V(G, D_G^*) &amp;= \mathbb{E}_{x \sim p_{\text{data}}} \left[\log D_G^*(x)\right] + \mathbb{E}_{z \sim r} \left[\log(1 - D_G^*(G(z)))\right] \\
        &amp;= \mathbb{E}_{x \sim p_{\text{data}}} \left[\log D_G^*(x)\right] + \mathbb{E}_{x \sim p_g} \left[\log(1 - D_G^*(x))\right] \\
        &amp;= \mathbb{E}_{x \sim p_{\text{data}}} \left[\log \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}\right] + \mathbb{E}_{x \sim p_g} \left[\frac{p_g(x)}{p_{\text{data}}(x) + p_g(x)}\right] \\
        \end{split}
    \end{equation}

    By definition of KL divergence $\text{KL}(P \| Q) = \mathbb{E}_{x \sim P} \left[\log \frac{p(x)}{q(x)}\right]$, we have:
    \begin{equation}
    \begin{split}
    V(G, D_G^*) &amp;= \mathbb{E}_{x \sim p_{\text{data}}} \left[\log \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}\right] + \mathbb{E}_{x \sim p_g} \left[\log \frac{p_g(x)}{p_{\text{data}}(x) + p_g(x)}\right] \\
    &amp;= -\log4 + \text{KL}\left(p_{\text{data}} \| \frac{p_\text{data} + p_g}{2}\right) + \text{KL}\left(p_g \| \frac{p_{\text{data}} + p_g}{2}\right) \\
    &amp;= -\log4 + 2 \cdot \text{JSD}(p_{\text{data}} \| p_g) \\
    &amp; \geq -\log4
    \end{split}
    \end{equation}

    The equality holds if and only if $p_{\text{data}} = p_g$.
\end{question}

\section{Adversarial Attacks}

\begin{question}[Principle of Generating Adversarial Attacks]
    \begin{equation}
        \max_{\left\|x_{\text{adv}}-x\right\|_{\infty} \leq \epsilon} \mathcal{L}(C(x_{\text{adv}}), y)
    \end{equation}

    where $C$ is the composition of $h$ and $f$.
\end{question}

\begin{question}[Different Solvers] to optimize the adversarial attack.
    \begin{itemize}
        \item \textbf{Zero-Order Solvers} (only access to the output of NN)
        \begin{itemize}
            \item Black-box attack
        \end{itemize}
        \item \textbf{First-Order Solvers} (access to the gradient of NN)
        \begin{itemize}
            \item White-box attack
            \item Fast Gradient Sign Method (FGSM), BIM, PGD, CW attack, \dots
        \end{itemize}
        \item \textbf{Second-Order Solvers} (access to the Hessian matrix)
        \begin{itemize}
            \item White-box attack
            \item L-BFGS attack
        \end{itemize}
    \end{itemize}
\end{question}

\begin{question}[Holder Inequality]
    For any $p, q \geq 1$ such that $\frac{1}{p} + \frac{1}{q} = 1$,
    \begin{equation} \label{eq:holder-inequality}
        \left\|x\right\|_p \cdot \left\|y\right\|_q \geq \left\langle x, y \right\rangle
    \end{equation}
    where $\left\langle x, y \right\rangle$ is the inner product.

    $\| \cdot \|_p$ and $\| \cdot \|_q$ are also known as dual norms.
    \begin{itemize}
        \item $\| \cdot \|_2$ is self-dual.
        \item $\| \cdot \|_\infty$ and $\| \cdot \|_1$ are dual norms.
    \end{itemize}
\end{question}

\begin{question}[FGSM -- Fast Gradient Sign Method]
    White-box and non-targeted (maximize the loss w.r.t. the true label).

    Do linear expansion at $x$:
    \begin{equation}
        \mathcal{L}(C(x + \delta), y) \approx \underbrace{\mathcal{L}(C(x), y)}_{\text{constant}} + \nabla_x \mathcal{L}(C(x), y) \cdot \delta
    \end{equation}

    The problem reduces to:
    \begin{equation}
        \max_{\left\|\delta\right\|_{\infty} \leq \epsilon} \nabla_x \mathcal{L}(C(x), y) \cdot \delta
    \end{equation}

    Because of holder inequality \eqref{eq:holder-inequality}, we have:
    \begin{equation}
        \nabla_x \mathcal{L}(C(x), y) \cdot \delta \leq \left\|\delta\right\|_{\infty} \cdot \left\|\nabla_x \mathcal{L}(C(x), y)\right\|_1
        \leq \epsilon \cdot \left\|\nabla_x \mathcal{L}(C(x), y)\right\|_1
    \end{equation}

    Thus, the adversarial example is generated by:

    \begin{equation}
        x_{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x \mathcal{L}(C(x), y))
    \end{equation}

    where $\epsilon$ is the perturbation size.
\end{question}

\begin{question}[BIM -- Basic Iterative Method]
    BIM is an iterative version of FGSM.
    \begin{itemize}
        \item Initialize $x^{(0)} = x$.
        \item For $k = 1, 2, \dots, K$:
        \begin{equation}
            x^{(k)} = x^{(k-1)} + \gamma \cdot \text{sign}(\nabla_x \mathcal{L}(C(x^{(k-1)}), y))
        \end{equation}
    \end{itemize}

    Issues:
    \begin{itemize}
        \item By repeating, the pertubation size $\epsilon$ will become larger.
        \item For a pre-defined $\epsilon$, $x^{(k)}$ may not satisfy $\|x^{(k)} - x\|_\infty \leq \epsilon$.
    \end{itemize}
\end{question}

\begin{question}[PGD -- Projected Gradient Descent]
    To resolve the issue of BIM, PGD involves a truncation operation:
    \begin{itemize}
        \item Initialize $x^{(0)} = x + \delta, \text{ where } \delta \in (-\epsilon, \epsilon)$.
        \item For $k = 1, 2, \dots, K$:
        \begin{equation}
            x^{(k)} = \text{clip}_{(-\epsilon, \epsilon)}(x^{(k-1)} + \gamma \cdot \text{sign}(\nabla_x \mathcal{L}(C(x^{(k-1)}), y)))
        \end{equation}
    \end{itemize}

    where $\text{clip}_{(-\epsilon, \epsilon)}(x)$ projects $x$ back to the $\ell_\infty$ ball of radius $\epsilon$ around $x$.
\end{question}

\begin{question}[Targeted PGD Attack]
    Objective:

    \begin{itemize}
        \item \textbf{Untargeted}: \begin{equation}
            \max_{\left\|\delta\right\|_{\infty} \leq \epsilon} \mathcal{L}(C(x + \delta), y_{\text{true}})
        \end{equation}
        \item \textbf{Targeted}: \begin{equation}
            \begingroup\color{red}\min_{\left\|\delta\right\|_{\infty} \leq \epsilon}\endgroup \mathcal{L}(C(x + \delta), \begingroup\color{red}y_{\text{target}}\endgroup)
        \end{equation}
    \end{itemize}
\end{question}

\section{Adversarial Robustness}

\begin{question}[Defense Mechanisms]
    Categorized into two types:
    \begin{itemize}
        \item \textbf{Gradient Masking}: hide the gradients and make first-order attacks fail
        \begin{itemize}
            \item \textbf{Shasttered Gradients}
            By applying a non-smooth or non-differentiable preprocessor $g$ to the inputs,
            and then training a DNN model $f$ on the preprocessed inputs $g(x)$.

            \item \textbf{Stochastic/Randomized Gradients}
            Apply some form of randomization of the DNN model.
            E.g. train a set of classifiers and during the testing phase randomly select one classifier to predict the labels.
        \end{itemize}
        \item \textbf{Adversarial Training}: 
        \begin{equation}
            \min_{C} \mathbb{E}_{(x, y) \sim \mathcal{D}^n} \left[ \max_{\left\|\delta\right\|_{\infty} \leq \epsilon} \mathcal{L}(C(x + \delta), y) \right]
        \end{equation}
    \end{itemize}
\end{question}

\begin{question}[Trade-Off between Natural and Robust Error]
    \begin{equation}
    \min_f R_{\text{nat}}(f) + R_{\text{rob}}(f) / \lambda
    \end{equation}

    \begin{equation}
        \begin{split}
        R_{\text{nat}}(f) &amp;\coloneq \Pr_{x,y \sim \mathcal{D}} \{f(x) y \leq 0\} \\
        &amp;= \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \mathbb{I}(f(x) y \leq 0) \right] \\
        \end{split}
    \end{equation}
    \begin{equation}
        \begin{split}
        R_{\text{rob}}(f) &amp;\coloneq \Pr_{x,y \sim \mathcal{D}} \{\exists \delta \in B_\epsilon(x) \text{ s.t. } f(x + \delta) y \leq 0\} \\
        &amp;= \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\left\|\delta\right\|_{\infty} \leq \epsilon} \mathbb{I}(f(x + \delta) y \leq 0) \right] \\
        \end{split}
    \end{equation}

    Approximate by a differentiable surrogate loss $\Phi$:
    \begin{equation}
        R_{\text{nat}}(f) \approx \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \Phi(f(x) y) \right]
    \end{equation}
\end{question}

\begin{question}[TRADES]
    \begin{equation}
        \min_f \underbrace{\left[ \underbrace{\mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \Phi(f(x) y) \right]}_{\text{minimize diff btw } f(x) \text{ and } y \text{ for accuracy}} + \underbrace{\mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\left\|\delta\right\|_{\infty} \leq \epsilon} \Phi(f(x + \delta) f(x)) \right]}_{\text{minimize diff btw }f(x) \text{ and } f(x+\delta){ for robustness}} / \lambda \right]}_{\text{TRADES Loss}}
    \end{equation}
\end{question}

\section{Self-Supervised Learning}

\begin{question}[Contrastive Learning]
    Loss:

    \begin{equation}
        \max_\Theta \Pr_1 = \frac{\exp(z_1)}{\sum_j \exp(z_j)}
    \end{equation}
\end{question}

\end{document}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cs-848-advanced-topics-in-database-algorithmic-aspects-of-database-query-processing"><a class="header" href="#cs-848-advanced-topics-in-database-algorithmic-aspects-of-database-query-processing">CS 848: Advanced Topics in Database: Algorithmic Aspects of Database Query Processing</a></h1>
<blockquote>
<p>Fall 25</p>
</blockquote>
<p>https://cs.uwaterloo.ca/~xiaohu/courses/CS848/CS848-F25.html</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="notes-for-foundations-of-databases"><a class="header" href="#notes-for-foundations-of-databases">Notes for <em>Foundations of Databases</em></a></h1>
<p><a href="http://webdam.inria.fr/Alice/">Foundations of Databases (The Alice Book)</a></p>
<p>The Alice Book is helpful for understanding the basic notations in database theory.
I recommend to read the first two chapters to get familiar with the notations used in this course.</p>
<p>That said, the book is not required.
In fact, paying real attention to the first several lectures should be sufficient to survive the course, and it turns out to be interesting.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-relational-model"><a class="header" href="#the-relational-model">The Relational Model</a></h1>
<!-- prettier-ignore-start -->
<blockquote>
<p><em>Mathjax</em> may have <a href="https://meta.stackexchange.com/questions/384924/i-think-there-is-a-mathjax-bug-for-mathcal-typesetting-on-some-platforms">issues</a> to display math symbols in particular styles, such as \(\mathcal{A}\).<br />
You can try to mitigate this by changing the <em>Math Renderer</em> to <em>Common HTML</em> (right click on any math expressions):
<img src="notes/cs848/foundations/../../../mathjax.png" alt="mathjax" />
\( \newcommand{\att}{\textbf{att}} \)
\( \newcommand{\attTotalOrder}{\leq_{\att}} \)
\( \newcommand{\dom}{\textbf{dom}} \)
\( \newcommand{\Dom}{\textit{Dom}} \)
\( \newcommand{\relname}{\textbf{relname}} \)
\( \newcommand{\sort}{\textit{sort}} \)
\( \newcommand{\arity}{\textit{arity}} \)
\( \newcommand{\finPowerSet}{\mathcal{P}^{\text{fin}}} \)
\( \newcommand{\dbschema}[1]{\textbf{#1}} \)
\( \newcommand{\dbinst}[1]{\textbf{#1}} \)
\( \newcommand{\rel}[1]{\textit{#1}} \)
\( \newcommand{\attname}[1]{\textit{#1}} \)
\( \newcommand{\varSet}{\textbf{var}} \)</p>
</blockquote>
<!-- prettier-ignore-end -->
<h2 id="informal-terminology"><a class="header" href="#informal-terminology">Informal Terminology</a></h2>
<ul>
<li><em>Relation</em>: table
<ul>
<li><em>Relation Name</em>: name of the table</li>
</ul>
</li>
<li><em>Attribute</em>: column name</li>
<li><em>Tuple/Record</em>: line in the table</li>
<li><em>Domain</em>: set of constants that entries of tuples can take</li>
<li><em>Database Schema</em>: specifying the structure of the database</li>
<li><em>Database Instance</em>: specifying the actual content in the database</li>
</ul>
<h2 id="formal-definitions"><a class="header" href="#formal-definitions">Formal Definitions</a></h2>
<ul>
<li>\(\att\): a countably infinite set of <em>attributes</em>
<ul>
<li>assume \(\att\) is fixed</li>
</ul>
</li>
<li>\(\attTotalOrder\): a total order on \(\att\)
<ul>
<li>the elements of a set of attributes \(U\) are written according to \(\attTotalOrder\) unless otherwise specified</li>
</ul>
</li>
<li>\(\dom\): a countably infinite set of <em>domains</em> (disjoint from \(\att\))
<ul>
<li>assume \(\dom\) is fixed</li>
<li>a <em>constant</em> is an element of \(\dom\)</li>
<li>for most cases, the same domain of values is used for all attributes;
<ul>
<li>otherwise, assume a mapping \(\Dom\) on \(\att\), where \(\Dom(A)\) is a set called the domain of attribute \(A\)</li>
</ul>
</li>
</ul>
</li>
<li>\(\relname\): a countably infinite set of <em>relation names</em> (disjoint from \(\att\) and \(\dom\))</li>
<li>\(\sort\): a function from \(\relname\) to \(\finPowerSet(\att)\) (the finitary powerset of \(\att\))
<ul>
<li>\(\sort^{-1}(U)\) is inifinite for each (possibly empty) finite set of attributes \(U\)
<ul>
<li>which allows multiple relations to have the same set of attributes</li>
</ul>
</li>
<li>\(\sort(R)\) is called the \(\sort\) of a relation name \(R\)</li>
<li>\(\arity(R) = |\sort(R)|\): the \(\arity\) of relation name \(R\)</li>
</ul>
</li>
<li>\(R\): a relation name or a <em>relation schema</em>
<ul>
<li>Alternative notations:
<ul>
<li>\(R[U]\): indicating \(\sort(R) = U\)</li>
<li>\(R[n]\): indicating \(\arity(R) = n\)</li>
</ul>
</li>
</ul>
</li>
<li>\(\dbschema{R}\): a <em>database schema</em>
<ul>
<li>a nonempty finite set of relation names</li>
<li>might be written as \(\dbschema{R} = \{R_1[U_1], \ldots, R_n[U_n]\}\)</li>
</ul>
</li>
</ul>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<p>Table <em>Movies</em></p>
<div class="table-wrapper"><table><thead><tr><th>Title</th><th>Director</th><th>Actor</th></tr></thead><tbody>
<tr><td>A</td><td>X</td><td>Y</td></tr>
</tbody></table>
</div>
<p>Table <em>Location</em></p>
<div class="table-wrapper"><table><thead><tr><th>Theater</th><th>Address</th><th>Phone</th></tr></thead><tbody>
<tr><td>T1</td><td>Addr1</td><td>P1</td></tr>
</tbody></table>
</div>
<p>Table <em>Pariscope</em></p>
<div class="table-wrapper"><table><thead><tr><th>Theater</th><th>Title</th><th>Schedule</th></tr></thead><tbody>
<tr><td>T1</td><td>A</td><td>9:00</td></tr>
</tbody></table>
</div>
<ul>
<li>The database schema \(\dbschema{CINEMA} = \{ \rel{Movies}, \rel{Location}, \rel{Pariscope} \}\)</li>
<li>The sorts of the relation names
<ul>
<li>\(\sort(\rel{Movies}) = \{ \attname{Title}, \attname{Director}, \attname{Actor} \}\)</li>
<li>\(\sort(\rel{Location}) = \{ \attname{Theater}, \attname{Address}, \attname{Phone} \}\)</li>
<li>\(\sort(\rel{Pariscope}) = \{ \attname{Theater}, \attname{Title}, \attname{Schedule} \}\)</li>
</ul>
</li>
</ul>
<h2 id="named-vs-unnamed-attributestuples"><a class="header" href="#named-vs-unnamed-attributestuples">Named v.s. Unnamed Attributes/Tuples</a></h2>
<h3 id="named-perspective"><a class="header" href="#named-perspective">Named Perspective</a></h3>
<p>E.g. \( \langle A:5, B:3 \rangle \)</p>
<p>a <em>tuple</em> over a finite set of attributes \(U\) (or over a relation schema \(R[U]\))
is a total mapping (viewed as a function) \(u\) from \(U\) to \(\dom\).</p>
<ul>
<li>\(u(U)\): \(u\) is a tuple over \(U\)</li>
<li>\(u(A)\): the value of \(u\) on an attribute \(A\) in \(U\) (\(A \in U\))</li>
<li>\(u[V] = u \vert_V\): the restriction of \(u\) to a subset \(V \subseteq U\), i.e.,
\(u[V]\) denotes the tuple \(v\) over \(V\) such that \(v(A) = u(A)\) for all \(A \in V\)</li>
</ul>
<h3 id="unnamed-perspective"><a class="header" href="#unnamed-perspective">Unnamed Perspective</a></h3>
<p>E.g. \( \langle 5, 3 \rangle \)</p>
<p>a <em>tuple</em> is an ordered <em>n</em>-tuple (\(n \geq 0\)) of constants (i.e., an element of the Cartesian product \(\dom^n\))</p>
<ul>
<li>\(u(i)\): the \(i\)-th coordinate of \(u\)</li>
</ul>
<h3 id="correspondence"><a class="header" href="#correspondence">Correspondence</a></h3>
<p>Because of the total order \(\attTotalOrder\), a tuple \(\langle A_1:a_1, A_2:a_2 \rangle\) (defined as a function)
can be viewed as an ordered tuple with \((A_1:a_1)\) as a first component and \((A_2:a_2)\) as a second component.
Ignoring the names, this tuple can be viewed as the ordered tuple \(\langle a_1, a_2 \rangle\).</p>
<p>Conversely, the ordered tuple \(t = \langle a_1, a_2 \rangle\) can be viewed as a function over the set of integers \(\{1, 2\}\)
with \(t(i) = a_i\) for each \(i\).</p>
<h2 id="conventional-vs-logic-programming-relational-model"><a class="header" href="#conventional-vs-logic-programming-relational-model">Conventional v.s. Logic Programming Relational Model</a></h2>
<h3 id="conventional-perspective"><a class="header" href="#conventional-perspective">Conventional Perspective</a></h3>
<p>a <em>relation (instance)</em> over a relation schema \(\rel{R}[U]\) is a finite set of tuples \(I\) with sort \(U\).</p>
<p>a <em>database instance</em> of database schema \(\dbschema{R}\) is a mapping \(\dbinst{I}\) with domain \(\dbschema{R}\),
such that \(\dbinst{I}(\rel{R})\) is a relation over \(\rel{R}\) for each \(\rel{R} \in \dbschema{R}\).</p>
<h3 id="logic-programming-perspective"><a class="header" href="#logic-programming-perspective">Logic Programming Perspective</a></h3>
<p>This perspective is used primarily with the ordered-tuple perspective on tuples.</p>
<p>A <em>fact</em> over \(\rel{R}\) is an expression of the form \(\rel{R}(a_1, \ldots, a_n)\),
where \(a_i \in \dom\) for \(i \in [1, n]\).<br />
If \(u = \langle a_1, \ldots, a_n \rangle\), we sometimes write \(\rel{R}(u)\) for \(\rel{R}(a_1, \ldots, a_n)\).</p>
<p>a <em>relation (instance)</em> over a relation schema \(\rel{R}\) is a finite set of facts over \(\rel{R}\).</p>
<p>a <em>database instance</em> of database schema \(\dbschema{R}\) is a finite set \(\dbinst{I}\) that is
the union of relation instances over \(\rel{R}\) for all \(\rel{R} \in \dbschema{R}\).</p>
<h3 id="examples-1"><a class="header" href="#examples-1">Examples</a></h3>
<p>Assume \(\sort(R) = AB, \sort(S) = A\).</p>
<h4 id="named-and-conventional"><a class="header" href="#named-and-conventional">Named and Conventional</a></h4>
<p>\[
\begin{split}
I(R) = \{ f_1, f_2, f_3 \} \quad &amp;&amp; \\
&amp; f_1(A) = a &amp;\quad f_1(B) = b \\
&amp; f_2(A) = c &amp;\quad f_2(B) = b \\
&amp; f_3(A) = a &amp;\quad f_3(B) = a \\
I(S) = \{g\} \quad &amp;&amp; \\
&amp; g(A) = d &amp;
\end{split}
\]</p>
<h4 id="unnamed-and-conventional"><a class="header" href="#unnamed-and-conventional">Unnamed and Conventional</a></h4>
<p>\[
\begin{split}
I(R) = \{ \langle a, b \rangle, \langle c, b \rangle, \langle a, a \rangle \} \quad &amp;&amp; \\
I(S) = \{ \langle d \rangle \} \quad &amp;&amp;
\end{split}
\]</p>
<h4 id="named-and-logic-programming"><a class="header" href="#named-and-logic-programming">Named and Logic Programming</a></h4>
<p>\[
\{ R(A:a, B:b), R(A:c, B:b), R(A:a, B:a), S(A:d) \}
\]</p>
<h4 id="unnamed-and-logic-programming"><a class="header" href="#unnamed-and-logic-programming">Unnamed and Logic Programming</a></h4>
<p>\[
\{ R(a, b), R(c, b), R(a, a), S(d) \}
\]</p>
<h2 id="variables"><a class="header" href="#variables">Variables</a></h2>
<p>an infinite set of <em>variables</em> \(\varSet\) will be used to range over elements of \(\dom\).</p>
<!-- prettier-ignore-start -->
<ul>
<li>a <em>free tuple</em> over \(U\) or \(\rel{R}(U)\) is a function \(u\) from \(U\) to \(\dom \cup \varSet\)</li>
<li>an <em>atom</em> over \(\rel{R}\) is an expression of the form \(\rel{R}(e_1, \ldots, e_n)\),
where \(n = \arity(\rel{R})\) and each <em>term</em> \(e_i \in \dom \cup \varSet\)</li>
<li>a <em>ground atom</em> is an atom with no variables, i.e., a <em>fact</em> (see <a href="notes/cs848/foundations/01.the-relation-model.html#logic-programming-perspective">Logic Programming Perspective</a>)</li>
</ul>
<!-- prettier-ignore-end -->
<ul>
<li><a href="notes/cs848/foundations/01.the-relation-model.html#notations">Notations</a></li>
</ul>
<h2 id="notations"><a class="header" href="#notations">Notations</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Object</th><th>Notation</th></tr></thead><tbody>
<tr><td>Constants</td><td>\(a, b, c\)</td></tr>
<tr><td>Variables</td><td>\(x, y\)</td></tr>
<tr><td>Sets of Variables</td><td>\(X, Y\)</td></tr>
<tr><td>Terms</td><td>\(e\)</td></tr>
<tr><td>Attributes</td><td>\(A, B, C\)</td></tr>
<tr><td>Sets of Attributes</td><td>\(U, V, W\)</td></tr>
<tr><td>Relation Names</td><td>\(\rel{R}, \rel{S}; \rel{R}[U], \rel{S}[V] \)</td></tr>
<tr><td>Database Schemas</td><td>\(\dbschema{R}, \dbschema{S} \)</td></tr>
<tr><td>Tuples</td><td>\(t, s\)</td></tr>
<tr><td>Free Tuples</td><td>\(u, v, w\)</td></tr>
<tr><td>Facts</td><td>\(\rel{R}(a_1, \ldots, a_n), R(t)\)</td></tr>
<tr><td>Atoms</td><td>\(\rel{R}(e_1, \ldots, e_n), R(u)\)</td></tr>
<tr><td>Relation Instances</td><td>\(I, J\)</td></tr>
<tr><td>Database Instances</td><td>\(\dbinst{I}, \dbinst{J} \)</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="conjunctive-queries"><a class="header" href="#conjunctive-queries">Conjunctive Queries</a></h1>
<!-- prettier-ignore-start -->
<blockquote>
<p><em>Mathjax</em> may have <a href="https://meta.stackexchange.com/questions/384924/i-think-there-is-a-mathjax-bug-for-mathcal-typesetting-on-some-platforms">issues</a> to display math symbols in particular styles, such as \(\mathcal{A}\).<br />
You can try to mitigate this by changing the <em>Math Renderer</em> to <em>Common HTML</em> (right click on any math expressions):
<img src="notes/cs848/foundations/../../../mathjax.png" alt="mathjax" />
\( \newcommand{\att}{\textbf{att}} \)
\( \newcommand{\attTotalOrder}{\leq_{\att}} \)
\( \newcommand{\dom}{\textbf{dom}} \)
\( \newcommand{\Dom}{\textit{Dom}} \)
\( \newcommand{\relname}{\textbf{relname}} \)
\( \newcommand{\sort}{\textit{sort}} \)
\( \newcommand{\arity}{\textit{arity}} \)
\( \newcommand{\finPowerSet}{\mathcal{P}^{\text{fin}}} \)
\( \newcommand{\dbschema}[1]{\textbf{#1}} \)
\( \newcommand{\dbinst}[1]{\textbf{#1}} \)
\( \newcommand{\rel}[1]{\textit{#1}} \)
\( \newcommand{\attname}[1]{\textit{#1}} \)
\( \newcommand{\varSet}{\textbf{var}} \)
\( \newcommand{\ans}{\textit{ans}} \)
\( \newcommand{\var}{\textit{var}} \)
\( \newcommand{\adom}{\textit{adom}} \)
\( \newcommand{\tableau}{\textbf{T}} \)
\( \newcommand{\free}{\textit{free}} \)
\( \newcommand{\body}{\text{body}} \)</p>
</blockquote>
<!-- prettier-ignore-end -->
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#logic-based-perspective">Logic-Based Perspective</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#rule-based-conjunctive-queries">Rule-Based Conjunctive Queries</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#definition">Definition</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#semantics">Semantics</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#properties">Properties</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#tableau-queries">Tableau Queries</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#examples">Examples</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#conjunctive-calculus">Conjunctive Calculus</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#conjunctive-calculus-formula">Conjunctive Calculus Formula</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#conjunctive-calculus-query">Conjunctive Calculus Query</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#semantics-1">Semantics</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#normal-form">Normal Form</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#expressiveness-of-query-languages">Expressiveness of Query Languages</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#incorporating-equality">Incorporating Equality</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#problem-1-infinite-answers">Problem 1: Infinite Answers</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#problem-2-unsatisfiable-queries">Problem 2: Unsatisfiable Queries</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#query-composition-and-views">Query Composition and Views</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#closure-under-composition">Closure under Composition</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#composition-and-user-views">Composition and User Views</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#algebraic-perspectives">Algebraic Perspectives</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#the-unnamed-perspective-the-spc-algebra">The Unnamed Perspective: The SPC Algebra</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#selection-horizontal-operator">Selection (‚ÄúHorizontal‚Äù Operator)</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#projection-vertical-operator">Projection (‚ÄúVertical‚Äù Operator)</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#cross-product-or-cartesian-product">Cross-Product (or Cartesian Product)</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#formal-inductive-definition">Formal Inductive Definition</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#unsatisifiable-queries">Unsatisifiable Queries</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#generalized-spc-algebra">Generalized SPC Algebra</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#normal-form-1">Normal Form</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#the-named-perspective-the-spjr-algebra">The Named Perspective: The SPJR Algebra</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#selection">Selection</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#projection">Projection</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#natural-join">(Natural) Join</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#renaming">Renaming</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#formal-inductive-definition-1">Formal Inductive Definition</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#normal-form-2">Normal Form</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#equivalence-theorem">Equivalence Theorem</a></li>
</ul>
</li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#adding-union">Adding Union</a>
<ul>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#nonrecursive-datalog-program">Nonrecursive Datalog Program</a></li>
<li><a href="notes/cs848/foundations/02.conjunctive-queries.html#union-and-the-conjunctive-calculus">Union and the Conjunctive Calculus</a></li>
</ul>
</li>
</ul>
<p>a query (mapping) is <em>from</em> (or <em>over</em>) its input schema and <em>to</em> its output schema.</p>
<!-- prettier-ignore-start -->
<ul>
<li><em>query</em>: a syntactic object</li>
<li><em>query mapping</em>: a function defined by a query interpreted under a specified semantics
<ul>
<li>its domain: the family of all instances of an input schema</li>
<li>its range: the family of instances of an output schema</li>
<li>we often blur query and query mapping when the meaning is clear from context</li>
</ul>
</li>
<li><em>input schema</em>: a specified relation or database schema</li>
<li><em>output schema</em>: a relation schema or database schema
<ul>
<li>For a relation schema, the relation name may be specified as part of the query syntax or by the context</li>
</ul>
</li>
<li>\(q_1 \equiv q_2\) denotes two queries \(q_1\) and \(q_2\) over \(\dbschema{R}\) are <em>equivalent</em>,
i.e., they have the same output schema and \(q_1(\dbinst{I}) = q_2(\dbinst{I})\) for each instance \(\dbinst{I}\) over \(\dbschema{R}\).</li>
</ul>
<!-- prettier-ignore-end -->
<h2 id="logic-based-perspective"><a class="header" href="#logic-based-perspective">Logic-Based Perspective</a></h2>
<p>Three versions of conjunctive queries:</p>
<ol>
<li>Rule-Based Conjunctive Queries</li>
<li>Tableau Queries</li>
<li>Conjunctive Calculus</li>
</ol>
<h3 id="rule-based-conjunctive-queries"><a class="header" href="#rule-based-conjunctive-queries">Rule-Based Conjunctive Queries</a></h3>
<h4 id="definition"><a class="header" href="#definition">Definition</a></h4>
<!-- prettier-ignore-start -->
<p>A <em>rule-based conjunctive query</em> (or often more simply called <em>rules</em>) \(q\) over a relation schema \(\dbschema{R}\) is an expression of the form
\[
\ans(u) \leftarrow \rel{R}_1(u_1), \ldots, \rel{R}_n(u_n)
\]
where \(n \geq 0\), \(\rel{R}_1, \ldots, \rel{R}_n\) are relation names in \(\dbschema{R}\);
\(\ans\) is a relation name not in \(\dbschema{R}\);
and \(u, u_1, \ldots, u_n\) are free tuples (i.e., may use either variables or constants).</p>
<ul>
<li>\(\var(q)\): the set of variables occurring in \(q\).</li>
<li><em>body</em>: \(\rel{R}_1(u_1), \ldots, \rel{R}_n(u_n)\)</li>
<li><em>head</em>: \(\ans(u)\)</li>
<li><em>range restricted</em>: each variable occurring in the head also occurs in the body
<ul>
<li>all conjunctive queries considered here are range restricted</li>
</ul>
</li>
<li><em>valuation</em>: a valuation \(v\) over \(V\), a finite subset of \(\varSet\), is a total function \(v\) from \(V\) to \(\dom\) (of constants)
<ul>
<li>extended to be identity on \(\dom\) (so that the domain can contain both variables and constants)</li>
<li>extended to map free tuples to tuples (so that the domain/range can be in the tuple form)</li>
</ul>
</li>
</ul>
<!-- prettier-ignore-end -->
<h4 id="semantics"><a class="header" href="#semantics">Semantics</a></h4>
<p>Let \(q\) be the query, and let \(\dbinst{I}\) be a database instance of schema \(\dbschema{R}\).
The <em>image</em> of \(\dbinst{I}\) under \(q\) is:</p>
<!-- prettier-ignore-start -->
<p>\[
q(\dbinst{I}) = \{ v(u) \mid v \text{ is a valuation over } \var(q) \text{ and } v(u_i) \in \dbinst{I}(\rel{R}_i) \text{ for each } i \in [1, n] \}
\]</p>
<ul>
<li><em>active domain</em>
<ul>
<li>of a database instance \(\dbinst{I}\), denoted \(\adom(\dbinst{I})\), is the set of constants that occur in \(\dbinst{I}\)</li>
<li>of a relation instance \(I\), denoted \(\adom(I)\), is the set of constants that occur in \(I\)</li>
<li>of a query \(q\), denoted \(\adom(q)\), is the set of constants that occur in \(q\)</li>
<li>\(\adom(q, \dbinst{I})\) is an abbreviation for \(\adom(q) \cup \adom(\dbinst{I})\)
<ul>
<li>\(\adom(q(\dbinst{I})) \subseteq \adom(q, \dbinst{I})\)</li>
</ul>
</li>
</ul>
</li>
<li><em>extensional</em> relations: relations in the body of the query, i.e., \(\rel{R}_1, \ldots, \rel{R}_n\)
<ul>
<li>because they are known/provided by the input instance \(\dbinst{I}\)</li>
</ul>
</li>
<li><em>intensional</em> relation: the relation in the head of the query, i.e., \(\ans\)
<ul>
<li>because it is not stored and its value is computed on request by the query</li>
</ul>
</li>
<li><em>extensional database (edb)</em>: a database instance associated with the extensional relations</li>
<li><em>intensional database (idb)</em>: the rule itself</li>
<li><em>idb relation</em>: the relation defined by the idb</li>
</ul>
<!-- prettier-ignore-end -->
<h4 id="properties"><a class="header" href="#properties">Properties</a></h4>
<p>Conjunctive queries are:</p>
<ul>
<li><em>monotonic</em>: a query \(q\) over \(\dbschema{R}\) is monotonic if for each \(\dbinst{I}, \dbinst{J}\) over \(\dbschema{R}\),
\(\dbinst{I} \subseteq \dbinst{J} \implies q(\dbinst{I}) \subseteq q(\dbinst{J})\)</li>
<li><em>satisfiable</em>: a query \(q\) is satisfiable if there exists a database instance \(\dbinst{I}\) such that \(q(\dbinst{I}) \neq \emptyset\)</li>
</ul>
<h3 id="tableau-queries"><a class="header" href="#tableau-queries">Tableau Queries</a></h3>
<p>A <em>tableau query</em> is simply a pair \((\tableau, u)\) where \(\tableau\) is a tableau
and each variable in \(u\) also occurs in \(\tableau\).</p>
<p>This is closest to the visual form provided by Query-By-Example (QBE).</p>
<ul>
<li><em>summary</em>: the free tuple \(u\) representing the tuples included in the answer to the query</li>
<li><em>embedding</em>: a valuation \(v\) for the variables occurring in \(\tableau\) such that \(v(\tableau) \subseteq \dbinst{I}\)
<ul>
<li>the output of \((\tableau, u)\) on \(\dbinst{I}\) consists of all tuples \(v(u)\) for each embedding \(v\) of \(\tableau\) into \(\dbinst{I}\)</li>
</ul>
</li>
<li><em>typed</em>: a tableau query \(q = (\tableau, u)\) under the named perspective, where \(\tableau\) is
over relation schema \(R\) and \(\sort(u) \subseteq \sort(R)\), is typed if no variable of \(\tableau\)
is associated with two distinct attributes in \(q\)</li>
</ul>
<h4 id="examples-2"><a class="header" href="#examples-2">Examples</a></h4>
<p>Table <em>Movies</em></p>
<div class="table-wrapper"><table><thead><tr><th>Title</th><th>Director</th><th>Actor</th></tr></thead><tbody>
<tr><td>\(x_{ti}\)</td><td>‚ÄúBergman‚Äù</td><td>\(x_{ac}\)</td></tr>
</tbody></table>
</div>
<p>Table <em>Location</em></p>
<div class="table-wrapper"><table><thead><tr><th>Theater</th><th>Address</th><th>Phone</th></tr></thead><tbody>
<tr><td>\(x_{th}\)</td><td>\(x_{ad}\)</td><td>\(x_p\)</td></tr>
</tbody></table>
</div>
<p>Table <em>Pariscope</em></p>
<div class="table-wrapper"><table><thead><tr><th>Theater</th><th>Title</th><th>Schedule</th></tr></thead><tbody>
<tr><td>\(x_{th}\)</td><td>\(x_{ti}\)</td><td>\(x_{s}\)</td></tr>
</tbody></table>
</div>
<p>The above tableau query is typed because each variable is associated with only one attribute:</p>
<ul>
<li>\(x_{ti}\): \(\attname{Title}\)</li>
<li>\(x_{ac}\): \(\attname{Actor}\)</li>
<li>\(x_{th}\): \(\attname{Theater}\)</li>
<li>\(x_{ad}\): \(\attname{Address}\)</li>
<li>\(x_p\): \(\attname{Phone}\)</li>
<li>\(x_{s}\): \(\attname{Schedule}\)</li>
</ul>
<p>However, the following tableau query is untyped:</p>
<p>Table <em>Movies</em></p>
<div class="table-wrapper"><table><thead><tr><th>Title</th><th>Director</th><th>Actor</th></tr></thead><tbody>
<tr><td>\(x_{ti}\)</td><td>\(x_{ac}\)</td><td>\(x_{ac}\)</td></tr>
</tbody></table>
</div>
<p>Because \(x_{ac}\) is associated with both \(\attname{Director}\) and \(\attname{Actor}\).</p>
<h3 id="conjunctive-calculus"><a class="header" href="#conjunctive-calculus">Conjunctive Calculus</a></h3>
<p>The conjunctive query
\[
\ans(u) \leftarrow \rel{R}_1(u_1), \ldots, \rel{R}_n(u_n)
\]</p>
<p>can be expressed as the following conjunctive calculus query that has the same semantics:
\[
\left\{ e_1, \ldots, e_m \vert \exists x_1, \ldots, x_k \left( \rel{R}(u_1) \wedge \ldots \wedge \rel{R}(u_n) \right) \right\}
\]
where \(x_1, \ldots, x_k\) are all the variables occurring in the body and not the head.</p>
<h4 id="conjunctive-calculus-formula"><a class="header" href="#conjunctive-calculus-formula">Conjunctive Calculus Formula</a></h4>
<p>Let \(\dbschema{R}\) be a relation schema.
A <em>(well-formed) formula</em> over \(\dbschema{R}\) for the conjunctive calculus is an expression having <strong>one of</strong> the following forms:</p>
<ol>
<li>an atom over \(\dbschema{R}\);</li>
<li>\(\varphi \wedge \psi \), where \(\varphi\) and \(\psi\) are formulas over \(\dbschema{R}\); or</li>
<li>\(\exists x \varphi\), where \(x\) is a variable and \(\varphi\) is a formula over \(\dbschema{R}\).</li>
</ol>
<p>An occurrence of a variable \(x\) in formula \(\varphi\) is <em>free</em> if:</p>
<ol>
<li>\(\varphi\) is an atom; or</li>
<li>\(\varphi = (\psi \wedge \xi)\) and the occurrence of \(x\) is free in \(\psi\) or \(\xi\); or</li>
<li>\(\varphi = \exists y \psi\), \(x \neq y\), and the occurrence of \(x\) is free in \(\psi\).</li>
</ol>
<p>\(\free(\varphi)\): the set of free variables in \(\varphi\).</p>
<p>An occurrence of a variable that is not free is <em>bound</em>.</p>
<h4 id="conjunctive-calculus-query"><a class="header" href="#conjunctive-calculus-query">Conjunctive Calculus Query</a></h4>
<p>A <em>conjunctive calculus query</em> over database schema \(\dbschema{R}\) is an expression of the form
\[
\{ e_1, \ldots, e_m \mid \varphi \}
\]
where</p>
<ul>
<li>\(\varphi\) is a conjunctive calculus formula,</li>
<li>\(\langle e_1, \ldots, e_m \rangle\) is a free tuple, and</li>
<li>the set of variables occurring in \(\langle e_1, \ldots, e_m \rangle\) is exactly \(\free(\varphi)\).</li>
</ul>
<p>For named perspective, the above query can be written as:
\[
\{ \langle e_1, \ldots, e_m \rangle : A_1, \ldots, A_m \mid \varphi \}
\]</p>
<blockquote>
<p>Note: In the previous chapter, a named tuple was usually denoted as:</p>
<p>\[ \langle A_1:a_1, \ldots, A_m:a_m \rangle \]</p>
<p>But here we denote the named tuple as:</p>
<p>\[ \langle e_1, \ldots, e_m \rangle : A_1, \ldots, A_m \]</p>
<p>Not sure which one is better or whether the author did this intentionally.</p>
</blockquote>
<h4 id="semantics-1"><a class="header" href="#semantics-1">Semantics</a></h4>
<h5 id="valuation"><a class="header" href="#valuation">Valuation</a></h5>
<p>A <em>valuation</em> over \(V \subset \varSet \) is a total function \(v\) from \(V\) to \(\dom\),
which can be viewed as a syntactic expression of the form:
\[
\{ x_1/a_1, \ldots, x_n/a_n \}
\]
where</p>
<ul>
<li>\(x_1, \ldots, x_n\) is a listing of \(V\),</li>
<li>\(a_i = v(x_i)\) for each \(i \in [1, n]\).</li>
</ul>
<h5 id="interpretation-as-a-set"><a class="header" href="#interpretation-as-a-set">Interpretation as a Set</a></h5>
<p>If \(x \not \in V\), and \(c \in \dom\),
then \(v \cup \{ x/c \}\) is the valuation over \(V \cup \{ x \}\)
that agrees with \(v\) on \(V\) and maps \(x\) to \(c\).</p>
<h5 id="satisfaction"><a class="header" href="#satisfaction">Satisfaction</a></h5>
<p>Let \(\dbschema{R}\) be a database schema, \(\varphi\) a conjunctive calculus formula over \(\dbschema{R}\),
and \(v\) a valuation over \(\free(\varphi)\).
Then \(\dbinst{I} \models \varphi[v]\), if</p>
<ol>
<li>\(\varphi = \rel{R}(u)\) is an atom and \(v(u) \in \dbinst{I}(\rel{R})\); or</li>
<li>\(\varphi = (\psi \wedge \xi)\) and \(\dbinst{I} \models \psi[v \vert_{\free(\psi)}]\) and \(\dbinst{I} \models \xi[v \vert_{\free(\xi)}]\); or</li>
<li>\(\varphi = \exists x \psi\) and there exists a constant \(c \in \dom\) such that
\(\dbinst{I} \models \psi[v \cup \{ x/c \}]\).</li>
</ol>
<h5 id="image"><a class="header" href="#image">Image</a></h5>
<!-- prettier-ignore-start -->
<p>Let \(q = \{ e_1, \ldots, e_m \mid \varphi \}\) be a conjunctive calculus query over \(\dbschema{R}\).
For an instance \(\dbinst{I}\) over \(\dbschema{R}\), the <em>image</em> of \(\dbinst{I}\) under \(q\) is:
\[
q(\dbinst{I}) = \{ v( \langle e_1, \ldots, e_m \rangle ) \mid v \text{ is a valuation over } \free(\varphi) \text{ and } \dbinst{I} \models \varphi[v] \}
\]</p>
<!-- prettier-ignore-end -->
<ul>
<li><em>active domain</em>
<ul>
<li>of a formula \(\varphi\), denoted \(\adom(\varphi)\), is the set of constants that occur in \(\varphi\)</li>
<li>\(\adom(\varphi, \dbinst{I})\) is an abbreviation for \(\adom(\varphi) \cup \adom(\dbinst{I})\)</li>
<li>If \(\dbinst{I} \models \varphi[v]\), then the range of \(v\) is contained in \(\adom(\dbinst{I})\)
<ul>
<li>to evaluate a conjunctive calculus query, one need only consider valuations with range constrained in \(\adom(\varphi, \dbinst{I})\),
i.e., only a finite number of them</li>
</ul>
</li>
</ul>
</li>
<li><em>equivalent</em>: conjunctive calculus formulas \(\varphi\) and \(\psi\) over \(\dbschema{R}\) are equivalent,
if
<ul>
<li>they have the same free variables and,</li>
<li>for each \(\dbinst{I}\) over \(\dbschema{R}\) and valuation \(v\) over \(\free(\varphi) = \free(\psi)\),
\(\dbinst{I} \models \varphi[v]\) if and only if \(\dbinst{I} \models \psi[v]\)</li>
</ul>
</li>
</ul>
<h4 id="normal-form"><a class="header" href="#normal-form">Normal Form</a></h4>
<p>\[
\exists x_1, \ldots, x_m \left( \rel{R}_1(u_1) \wedge \ldots \wedge \rel{R}_n(u_n) \right)
\]</p>
<p>Each conjunctive calculus query is equivalent to a conjunctive calculus query in normal form.</p>
<h3 id="expressiveness-of-query-languages"><a class="header" href="#expressiveness-of-query-languages">Expressiveness of Query Languages</a></h3>
<!-- prettier-ignore-start -->
<p>Let \(\mathcal{Q}_1\) and \(\mathcal{Q}_2\) be two query languages.</p>
<ul>
<li>\(\mathcal{Q}_1 \sqsubseteq \mathcal{Q}_2\):
\(\mathcal{Q}_1\) is <em>dominated</em> by \(\mathcal{Q}_2\)
(or \(\mathcal{Q}_2\) is <em>weaker</em> than \(\mathcal{Q}_1\)),
if for each query \(q_1 \in \mathcal{Q}_1\),
there exists a query \(q_2 \in \mathcal{Q}_2\) such that \(q_1 \equiv q_2\).</li>
<li>\(\mathcal{Q}_1 \equiv \mathcal{Q}_2\):
\(\mathcal{Q}_1\) and \(\mathcal{Q}_2\) are <em>equivalent</em>,
if \(\mathcal{Q}_1 \sqsubseteq \mathcal{Q}_2\) and \(\mathcal{Q}_2 \sqsubseteq \mathcal{Q}_1\).</li>
</ul>
<!-- prettier-ignore-end -->
<p>The rule-based conjunctive queries, tableau queries, and conjunctive calculus queries are all equivalent.</p>
<h2 id="incorporating-equality"><a class="header" href="#incorporating-equality">Incorporating Equality</a></h2>
<p>The conjunctive query</p>
<p>\[
\begin{split}
\ans(x_{th}, x_{ad}) \leftarrow &amp; \rel{Movies}(x_{ti}, \text{‚ÄúBergman‚Äù}, x_{ac}), \\
&amp; \qquad \rel{Location}(x_{th}, x_{ad}, x_p), \rel{Pariscope}(x_{th}, x_{ti}, x_s)
\end{split}
\]</p>
<p>can be expressed as:</p>
<p>\[
\begin{split}
\ans(x_{th}, x_{ad}) \leftarrow &amp; \rel{Movies}(x_{ti}, x_d, x_{ac}), x_d = \text{‚ÄúBergman‚Äù}, \\
&amp; \qquad \rel{Location}(x_{th}, x_{ad}, x_p), \rel{Pariscope}(x_{th}, x_{ti}, x_s)
\end{split}
\]</p>
<h3 id="problem-1-infinite-answers"><a class="header" href="#problem-1-infinite-answers">Problem 1: Infinite Answers</a></h3>
<p>Unrestricted rules with equality may yield infinite answers:
\[
\ans(x, y) \leftarrow \rel{R}(x), y = z
\]</p>
<p>A <em>rule-based conjunctive query with equality</em> is a <em>range-restricted</em> rule-based conjunctive query with equality.</p>
<h3 id="problem-2-unsatisfiable-queries"><a class="header" href="#problem-2-unsatisfiable-queries">Problem 2: Unsatisfiable Queries</a></h3>
<p>Consider the following query:
\[
\ans(x) \leftarrow \rel{R}(x), x = a, x = b
\]
where \(\rel{R}\) is a unary relation and \(a, b \in \dom\) with \(a \neq b\).</p>
<p>Each satisfiable rule with equality is equivalent to a rule without equality.<br />
No expressive power is gained if the query is satisfiable.</p>
<h2 id="query-composition-and-views"><a class="header" href="#query-composition-and-views">Query Composition and Views</a></h2>
<p>a <em>conjunctive query program</em> (with or without equality) is a sequence \(P\)
of rules having the form:
\[
\begin{split}
S_1(u_1) &amp; \leftarrow \text{body}_1 \\
S_2(u_2) &amp; \leftarrow \text{body}_2 \\
&amp; \vdots \\
S_m(u_m) &amp; \leftarrow \text{body}_m
\end{split}
\]
where</p>
<!-- prettier-ignore-start -->
<ul>
<li>each \(S_i\) is a distinct relation name not in \(\dbschema{R}\),</li>
<li>for each \(i \in [1, m]\), the only relation names that may occur in \(\body_i\) are in \(\dbschema{R} \cup \{ S_1, \ldots, S_{i-1} \}\).</li>
</ul>
<!-- prettier-ignore-end -->
<h3 id="closure-under-composition"><a class="header" href="#closure-under-composition">Closure under Composition</a></h3>
<p>If conjunctive query program \(P\) defines final relation \(S\),
then there is a conjunctive query \(q\),
possibly with equality,
such that on all input instances \(\dbinst{I}\),
\(q(\dbinst{I}) = P(\dbinst{I})(S)\).</p>
<p>If \(P\) is satisfiable, then there is a \(q\) without equality.</p>
<h3 id="composition-and-user-views"><a class="header" href="#composition-and-user-views">Composition and User Views</a></h3>
<p><em>views</em> are specified as queries (or query programs), which may be</p>
<ul>
<li><em>materialized</em>: a physical copy of the view is stored and maintained</li>
<li><em>virtual</em>: relevant information about the view is computed as needed
<ul>
<li>queries against the virtual view generate composed queries against the underlying database</li>
</ul>
</li>
</ul>
<h2 id="algebraic-perspectives"><a class="header" href="#algebraic-perspectives">Algebraic Perspectives</a></h2>
<h3 id="the-unnamed-perspective-the-spc-algebra"><a class="header" href="#the-unnamed-perspective-the-spc-algebra">The Unnamed Perspective: The SPC Algebra</a></h3>
<p>Unnamed conjunctive algebra:</p>
<ul>
<li><strong>S</strong>election (\(\sigma\))</li>
<li><strong>P</strong>rojection (\(\pi\))</li>
<li><strong>C</strong>ross-Product (or Cartesian Product, \(\times\))</li>
</ul>
<!-- prettier-ignore-start -->
<h4 id="selection-horizontal-operator"><a class="header" href="#selection-horizontal-operator">Selection (‚ÄúHorizontal‚Äù Operator)</a></h4>
<p>Primitive Forms: \(\sigma_{j=a}\) and \(\sigma_{j=k}\)<br />
where \(j, k\) are positive integers, and \(a \in \dom\)
(constants are usually surrounded by quotes).</p>
<p>\[
\sigma_{j=a}(I) = \{ t \in I \mid t(j) = a \}
\]</p>
<p>\(\sigma_{j=k}\) is sometimes called <em>atomic</em> selection.</p>
<h4 id="projection-vertical-operator"><a class="header" href="#projection-vertical-operator">Projection (‚ÄúVertical‚Äù Operator)</a></h4>
<p>General Form: \(\pi_{j_1, \ldots, j_n}\)<br />
where \(n \geq 0\) and \(j_1, \ldots, j_n\) are positive integers
(empty sequence is written \([\;]\)).</p>
<p>\[
\pi_{j_1, \ldots, j_n}(I) = \{ \langle t(j_1), \ldots, t(j_n) \rangle \mid t \in I \}
\]</p>
<h4 id="cross-product-or-cartesian-product"><a class="header" href="#cross-product-or-cartesian-product">Cross-Product (or Cartesian Product)</a></h4>
<p>\[
I \times J = \{ \langle t(1), \ldots, t(n), s(1), \ldots, s(m) \rangle \mid t \in I, s \in J \}
\]</p>
<h4 id="formal-inductive-definition"><a class="header" href="#formal-inductive-definition">Formal Inductive Definition</a></h4>
<p>Let \(\dbschema{R}\) be a relation schema.</p>
<p>There are two kinds of <em>base SPC (algebra) queries</em>:</p>
<ol>
<li><em>Input Relation</em>: Expression \(R\); with arity equal to \(\arity(R)\)</li>
<li><em>Unary Singleton Constant</em>: Expression \(\{\langle a \rangle \}\),
where \(a \in \dom\); with arity equal to 1</li>
</ol>
<p>The family of <em>SPC (algebra) queries</em> contains all above <em>base SPC queries</em> and,
for SPC queries \(q_1, q_2\) with arities \(\alpha_1, \alpha_2\), respectively,</p>
<ul>
<li><em>Selection</em>: \(\sigma_{j=1}(q_1)\) and \(\sigma_{j=k}(q_1)\),
whenever \(j, k \leq \alpha_1\) and \(a \in \dom\); these have arity \(\alpha_1\).</li>
<li><em>Projection</em>: \(\pi_{j_1, \ldots, j_n}(q_1)\),
where \(j_1, \ldots, j_n \leq \alpha_1\); this has arity \(n\).</li>
<li><em>Cross-Product</em>: \(q_1 \times q_2\); this has arity \(\alpha_1 + \alpha_2\).</li>
</ul>
<h4 id="unsatisifiable-queries"><a class="header" href="#unsatisifiable-queries">Unsatisifiable Queries</a></h4>
<p>E.g. \(\sigma_{1=a}(\sigma_{1=b}(R))\) where \(\arity(R) \geq 1\) and \(a \neq b\).</p>
<p>This is equivalent to \(q^\emptyset\).</p>
<h4 id="generalized-spc-algebra"><a class="header" href="#generalized-spc-algebra">Generalized SPC Algebra</a></h4>
<ul>
<li><em>Intersection</em> (\(\cap\)): is easily simulated by selection and cross-product.</li>
<li><em>Generalized Selection</em> (\(\sigma\)): permits the specification of multiple conditions.
<ul>
<li><em>Positive Conjunctive Selection Formula</em>: \(F = \gamma_1 \land \ldots \land \gamma_n (n \geq 1)\) where each \(\gamma_i\) is either \(j = k\) or \(j = a\)</li>
<li><em>Positive Conjunctive Selection Operator</em>: an expression of the form \(\sigma_F\), where \(F\) is a positive conjunctive selection formula</li>
<li>can be simulated by a sequence of selections</li>
</ul>
</li>
<li><em>Equi-Join</em> (\(\bowtie\)): a binary operator that combines cross-product and selection
<ul>
<li><em>Equi-Join Condition</em>: \(F = \gamma_1 \land \ldots \land \gamma_n (n \geq 1)\) where each \(\gamma_i\) is of the form \(j = k\)</li>
<li><em>Equi-Join Operator</em>: an expression of the form \(I \bowtie_F J\), where \(F\) is an equi-join condition</li>
<li>can be simulated by \(\sigma_{F‚Äô}(I \times J)\),
where \(F‚Äô\) is obtained from \(F\) by replacing each
condition \(j = k\) with \(j = \alpha(I) + k\)</li>
</ul>
</li>
</ul>
<h4 id="normal-form-1"><a class="header" href="#normal-form-1">Normal Form</a></h4>
<p>\[
\pi_{j_1, \ldots, j_n}( \{ \langle a_1 \rangle \} \times \ldots \times \{ \langle a_m \rangle \} \times \sigma_F(R_1 \times \ldots \times R_k) )
\]
where</p>
<ul>
<li>\(n \geq 0; m \geq 0\);</li>
<li>\(a_1, \ldots, a_m \in \dom\);</li>
<li>\(\{1, \ldots, m\} \subseteq \{j_1, \ldots, j_n\}\);</li>
<li>\(R_1, \ldots, R_k\) are relation names (repeats permitted);</li>
<li>\(F\) is a positive conjunctive selection formula.</li>
</ul>
<p>For each (generalized) SPC query, there is an equivalent SPC query in normal form.</p>
<p>The proof is based on repeated application of the following <em>equivalence-preserving SPC algebra rewrite rules</em> (or <em>transformations</em>):</p>
<ul>
<li><em>Merge-Select</em>:
\[
\sigma_{F}( \sigma_{F‚Äô}(q) ) \equiv \sigma_{F \land F‚Äô}(q)
\]</li>
<li><em>Merge-Project</em>:
\[
\pi_{\boldsymbol{j}}( \pi_{\boldsymbol{k}}(q) ) \equiv \pi_{\boldsymbol{l}}(q), \text{ where } l_i = k_{j_i} \text{ for each term } l_i \text{ in } \boldsymbol{l}
\]</li>
<li><em>Push-Select-Though-Project</em>:
\[
\sigma_F( \pi_{\boldsymbol{j}}(q) ) \equiv \pi_{\boldsymbol{j}}( \sigma_{F‚Äô}(q) ), \text{ where } F‚Äô \text{ is obtained from } F \text{ by replacing each } k \text{ with } j_k
\]</li>
<li><em>Push-Select-Though-Singleton</em>:
\[
\sigma_{1=j}(\langle a \rangle \times q) \equiv \langle a \rangle \times \sigma_{(j - 1) = a}(q)
\]</li>
<li><em>Associate-Cross</em>:
\[
(q_1 \times \ldots \times q_n) \times q \equiv q_1 \times \ldots \times q_n \times q
\]
\[
q \times (q_1 \times \ldots \times q_n) \equiv q_1 \times \ldots \times q_n \times q
\]</li>
<li><em>Commute-Cross</em>:
\[
(q \times q‚Äô) \equiv \pi_{\boldsymbol{j} \boldsymbol{j‚Äô}} (q‚Äô \times q)
\]
where \(\boldsymbol{j‚Äô} = [1, \ldots, \arity(q‚Äô)]\) and \(\boldsymbol{j} = [\arity(q‚Äô) + 1, \ldots, \arity(q‚Äô) + \arity(q)]\)</li>
<li><em>Push-Cross-Though-Select</em>:
\[
\sigma_F(q) \times q‚Äô \equiv \sigma_{F}(q \times q‚Äô)
\]
\[
q \times \sigma_F(q‚Äô) \equiv \sigma_{F‚Äô}(q \times q‚Äô)
\]
where \(F‚Äô\) is obtained from \(F\) by replacing each \(j\) with \(\arity(q) + j\)</li>
<li><em>Push-Cross-Though-Project</em>:
\[
\pi_{\boldsymbol{j}}(q) \times q‚Äô \equiv \pi_{\boldsymbol{j}}(q \times q‚Äô)
\]
\[
q \times \pi_{\boldsymbol{j}}(q‚Äô) \equiv \pi_{\boldsymbol{j‚Äô}}(q \times q‚Äô)
\]
where \(\boldsymbol{j‚Äô}\) is obtained from \(\boldsymbol{j}\) by replacing each \(j\) with \(\arity(q) + j\)</li>
</ul>
<h5 id="notation-for-rewriting"><a class="header" href="#notation-for-rewriting">Notation for Rewriting</a></h5>
<p>\[
\begin{split}
q &amp;\rightarrow_{\mathcal{S}} q‚Äô \quad \text{or} \\
\text{simply} \quad q &amp;\rightarrow q‚Äô
\end{split}
\]</p>
<ul>
<li>for a set \(\mathcal{S}\) of rewrite rules and algebra expressions \(q, q‚Äô\),</li>
<li>if \(\mathcal{S}\) is understood from the context,</li>
<li>if \(q‚Äô\) is the result of replacing a subexpression of \(q\)
according to one of the rules in \(\mathcal{S}\).</li>
</ul>
<p>\(\rightarrow^*_S\): the reflexive and transitive closure of \(\rightarrow\).</p>
<p>A family \(\mathcal{S}\) of rewrite rules is <em>sound</em>
if \(q \rightarrow_{\mathcal{S}} q‚Äô\) implies \(q \equiv q‚Äô\).
If \(\mathcal{S}\) is sound, then \(q \rightarrow^*_{\mathcal{S}} q‚Äô\) implies \(q \equiv q‚Äô\).</p>
<h3 id="the-named-perspective-the-spjr-algebra"><a class="header" href="#the-named-perspective-the-spjr-algebra">The Named Perspective: The SPJR Algebra</a></h3>
<p>Named conjunctive algebra:</p>
<ul>
<li><strong>S</strong>election (\(\sigma\))</li>
<li><strong>P</strong>rojection (\(\pi\)); repeats not permitted</li>
<li><strong>J</strong>oin (\(\bowtie\)); natural join</li>
<li><strong>R</strong>enaming (\(\delta\))</li>
</ul>
<h4 id="selection"><a class="header" href="#selection">Selection</a></h4>
<p>\(\sigma_{A=a}, \sigma_{A=B}\) where \(A, B \in \att, a \in \dom\)</p>
<p>These operators apply to any instance \(I\) with \(A, B \in \sort(I)\).</p>
<h4 id="projection"><a class="header" href="#projection">Projection</a></h4>
<p>\(\pi_{A_1, \ldots, A_n}\) where \(n \geq 0\) and \(A_1, \ldots, A_n \in \sort(I)\)</p>
<h4 id="natural-join"><a class="header" href="#natural-join">(Natural) Join</a></h4>
<p>\[
\begin{split}
I \bowtie J = \{ t \text{ over } V \cup W \vert &amp; \text{ for some } v \in I \text{ and } w \in J, \\
&amp; \quad t[V] = v \land t[W] = w \}
\end{split}
\]
where \(I\) and \(J\) have sorts \(V\) and \(W\), respectively.</p>
<ul>
<li>When \(\sort(I) = \sort(J)\), \(I \bowtie J = I \cap J\).</li>
<li>When \(\sort(I) \cap \sort(J) = \emptyset\), \(I \bowtie J = I \times J\).</li>
</ul>
<h4 id="renaming"><a class="header" href="#renaming">Renaming</a></h4>
<p>An <em>attribute renaming</em> for a finite set \(U\) of attributes
is a one-one mapping from \(U\) to \(\att\).</p>
<p>\[
\delta_f(I) = \{ v \text{ over } f[U] \vert \text{ for some } u \in I, v(f(A)) = u(A) \text{ for each } A \in U \}
\]
where</p>
<ul>
<li>input is an instance \(I\) over \(U\)</li>
<li>\(f\) is an attribute renaming for \(U\)
<ul>
<li>which can be described by \((A, f(A))\), where \(f(A) \neq A\)
<ul>
<li>usually written as \(A_1 A_2 \ldots A_n \rightarrow B_1 B_2 \ldots B_n\) to indicate that \(f(A_i) = B_i\) for each \(i \in [1, n] (n \geq 0)\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="formal-inductive-definition-1"><a class="header" href="#formal-inductive-definition-1">Formal Inductive Definition</a></h4>
<p>The <em>base SPJR algebra queries</em> are:</p>
<ol>
<li><em>Input Relation</em>: Expression \(R\); with sort equal to \(\sort(R)\)</li>
<li><em>Unary Singleton Constant</em>: Expression \(\{ \langle A:a \rangle \}\),
where \(a \in \dom\); with \(\sort = A\)</li>
</ol>
<p>The remainder of the syntax and semantics of the SPJR algebra is defined in analogy to those of the SPC algebra.</p>
<h4 id="normal-form-2"><a class="header" href="#normal-form-2">Normal Form</a></h4>
<p>\[
\pi_{B_1, \ldots, B_m}( \{ \langle A_1:a_1 \rangle \} \bowtie \ldots \bowtie \{ \langle A_m:a_m \rangle \} \bowtie \sigma_F(\delta_{f_1}(R_1) \bowtie \ldots \bowtie \delta_{f_k}(R_k)) )
\]
where</p>
<ul>
<li>\(n \geq 0; m \geq 0\);</li>
<li>\(a_1, \ldots, a_m \in \dom\);</li>
<li>\({A_1, \ldots, A_m}\) occurs in \({B_1, \ldots, B_n}\);</li>
<li>the \(A_i\)s are distinct;</li>
<li>\(R_1, \ldots, R_k\) are relation names (repeats permitted);</li>
<li>\(\delta_{f_i}\) is a renaming operator for \(\sort(R_i)\);</li>
<li>no \(A_i\) occur in any \(\delta_{f_j}(R_j)\);</li>
<li>the sorts of \(\delta_{f_1}(R_1), \ldots, \delta_{f_k}(R_k)\) are pairwise disjoint;</li>
<li>\(F\) is a positive conjunctive selection formula.</li>
</ul>
<p>For each SPJR query, there is an equivalent SPJR query in normal form.</p>
<!-- prettier-ignore-end -->
<h3 id="equivalence-theorem"><a class="header" href="#equivalence-theorem">Equivalence Theorem</a></h3>
<ul>
<li><em>Lemma</em>: The SPC and SPJR algebras are equivalent.</li>
<li><em>Equivalence Theorem</em>: The rule-based conjunctive queries, tableau queries, conjunctive calculus queries, satisifiable SPC algebra, and satisifiable SPJR algebra are all equivalent.</li>
</ul>
<h2 id="adding-union"><a class="header" href="#adding-union">Adding Union</a></h2>
<p>The following have equivalent expressive power:</p>
<ul>
<li>the nonrecursive datalog programs (with single relation target)</li>
<li>the SPCU queries</li>
<li>the SPJRU queries
<ul>
<li>union can only be applied to expressions having the same sort</li>
</ul>
</li>
</ul>
<h3 id="nonrecursive-datalog-program"><a class="header" href="#nonrecursive-datalog-program">Nonrecursive Datalog Program</a></h3>
<p>A <em>nonrecursive datalog program (nr-datalog program)</em> over schema \(\dbschema{R}\) is a set of rules:</p>
<p>\[
\begin{split}
S_1 &amp; \leftarrow \body_1 \\
S_2 &amp; \leftarrow \body_2 \\
&amp; \vdots \\
S_m &amp; \leftarrow \body_m
\end{split}
\]
where</p>
<ul>
<li>no relation name in \(\dbschema{R}\) occurs in  a rule head</li>
<li>the same relation name may appear in more than one rule head</li>
<li>there is some ordering \(r_1, \ldots, r_m\) of the rules such that
the relation name in the head of \(r_i\) does not occur in the body of any rule \(r_j\) with \(j \leq i\)</li>
</ul>
<h3 id="union-and-the-conjunctive-calculus"><a class="header" href="#union-and-the-conjunctive-calculus">Union and the Conjunctive Calculus</a></h3>
<p>Simply permitting disjunction (denoted \(\lor\)) in the formula of a conjunctive calculus along with conjunction can have serious consequences.</p>
<p>E.g.,</p>
<p>\[
q = \{ x, y, z \vert R(x, y) \lor R(y, z) \}
\]</p>
<p>The answer of \(q\) on nonempty instance \(I\) will be</p>
<p>\[
q(I)= (I \times \dom) \cup (\dom \times I)
\]</p>
<p>Because \(R(x, y)\) decides that the first two components in \(\langle x, y, z \rangle\) are in \(I\) and \(z\) can be anything in \(\dom\),
and \(R(y, z)\) decides that the last two components in \(\langle x, y, z \rangle\) are in \(I\) and \(x\) can be anything in \(\dom\).</p>


                        <footer id="footer" style="margin-top: 8em;">
                        </footer>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Set the theme for giscus comments -->
        <script>
            function normGiscusTheme (theme) {
                if (theme == "light") {
                    return "light";
                } else if (theme == "rust") {
                    return "noborder_light";
                } else {
                    return "transparent_dark";
                }
            }

            function changeGiscusTheme (theme) {
                theme = normGiscusTheme(theme);

                function sendMessage(message) {
                    const iframe = document.querySelector('iframe.giscus-frame');
                    if (!iframe) return;
                    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
                }

                sendMessage({
                    setConfig: {
                    theme: theme
                    }
                });
            }

            let giscusAttributes = {
                "src": "https://giscus.app/client.js",
                "data-repo": "xuhongxu96/xuhongxu96.github.io",
                "data-repo-id": "MDEwOlJlcG9zaXRvcnkxNDA0NTkxOTg=",
                "data-category": "Comments",
                "data-category-id": "DIC_kwDOCF88vs4CuW9z",
                "data-mapping": "og:title",
                "data-strict": "1",
                "data-reactions-enabled": "1",
                "data-emit-metadata": "0",
                "data-input-position": "bottom",
                "data-theme": normGiscusTheme(theme),
                "data-lang": "en",
                "data-loading": "lazy",
                "crossorigin": "anonymous",
                "async": "",
            };
            
            let giscusScript = document.createElement("script");
            Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
            document.getElementById("footer").appendChild(giscusScript);
        </script>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>


    </div>
    </body>
</html>
